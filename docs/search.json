[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Machine Learning from Scratch\n\n\n\n\n\n\nKai Stern\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\nThis project explores two methods for estimating the MNL model: Maximum Likelihood and a Bayesian MCMC approach.\n\n\n\nKai Stern\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nKai Stern\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nKai Stern\n\n\nApr 20, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project4/index.html",
    "href": "blog/project4/index.html",
    "title": "Machine Learning from Scratch",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n\n\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this.\n\n#Custom kmeans algorithm function\ndef kmeans_custom(X, k=3, max_iters=100, random_state=42):\n    np.random.seed(random_state)\n    n_samples, n_features = X.shape\n    \n    centroids = X[np.random.choice(n_samples, k, replace=False)]\n    history = [centroids.copy()]\n    \n    for i in range(max_iters):\n        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        \n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        \n        if np.allclose(centroids, new_centroids):\n            break\n            \n        centroids = new_centroids\n        history.append(centroids.copy())\n    \n    return labels, centroids, history\n\n\n#plot kmeans steps!\ndef plot_kmeans_steps(X, history, k, title=\"K-Means Steps\"):\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.ravel()\n    \n    steps_to_show = min(6, len(history))\n    colors = ['red', 'blue', 'green', 'purple', 'orange'][:k]\n    \n    for i in range(steps_to_show):\n        ax = axes[i]\n        \n        if i == 0:\n            ax.scatter(X[:, 0], X[:, 1], alpha=0.6, color='lightgray')\n            ax.scatter(history[i][:, 0], history[i][:, 1], c='red', marker='x', s=100, linewidths=3)\n            ax.set_title(f'Initial Centroids')\n        else:\n            distances = np.sqrt(((X - history[i-1][:, np.newaxis])**2).sum(axis=2))\n            labels = np.argmin(distances, axis=0)\n            \n            for j in range(k):\n                mask = labels == j\n                ax.scatter(X[mask, 0], X[mask, 1], c=colors[j], alpha=0.6, label=f'Cluster {j+1}')\n            \n            ax.scatter(history[i][:, 0], history[i][:, 1], c='black', marker='x', s=100, linewidths=3)\n            ax.set_title(f'Step {i}')\n        \n        ax.set_xlabel('Bill Length (mm)')\n        ax.set_ylabel('Flipper Length (mm)')\n        ax.grid(True, alpha=0.3)\n    \n    for i in range(steps_to_show, 6):\n        axes[i].set_visible(False)\n        \n    plt.tight_layout()\n    plt.suptitle(title, y=1.02, fontsize=16)\n    plt.show()\n\n\n#Helper functions to calculate metrics and compare \n#custom vs scikit learn kmeans outcomes\ndef calculate_wcss(X, labels, centroids):\n    wcss = 0\n    for i in range(len(centroids)):\n        cluster_points = X[labels == i]\n        if len(cluster_points) &gt; 0:\n            wcss += np.sum((cluster_points - centroids[i])**2)\n    return wcss\n\ndef analyze_clusters(X):\n    k_range = range(2, 8)\n    wcss_scores = []\n    silhouette_scores = []\n    \n    for k in k_range:\n        labels, centroids, _ = kmeans_custom(X, k=k, random_state=42)\n        wcss = calculate_wcss(X, labels, centroids)\n        wcss_scores.append(wcss)\n        sil_score = silhouette_score(X, labels)\n        silhouette_scores.append(sil_score)\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    ax1.plot(k_range, wcss_scores, 'bo-', linewidth=2, markersize=8)\n    ax1.set_xlabel('Number of Clusters (k)')\n    ax1.set_ylabel('Within-Cluster Sum of Squares')\n    ax1.set_title('Elbow Method for Optimal k')\n    ax1.grid(True, alpha=0.3)\n    \n    ax2.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n    ax2.set_xlabel('Number of Clusters (k)')\n    ax2.set_ylabel('Silhouette Score')\n    ax2.set_title('Silhouette Analysis')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return wcss_scores, silhouette_scores\n\n\ndef compare_implementations(X):\n    k = 3\n    \n    labels_custom, centroids_custom, _ = kmeans_custom(X, k=k, random_state=42)\n    \n    kmeans_sklearn = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans_sklearn.fit(X)\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    colors = ['red', 'blue', 'green']\n    \n    for i in range(k):\n        mask = labels_custom == i\n        ax1.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.6, label=f'Cluster {i+1}')\n    ax1.scatter(centroids_custom[:, 0], centroids_custom[:, 1], c='black', marker='x', s=100, linewidths=3)\n    ax1.set_title('Custom K-Means')\n    ax1.set_xlabel('Bill Length (mm)')\n    ax1.set_ylabel('Flipper Length (mm)')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    for i in range(k):\n        mask = kmeans_sklearn.labels_ == i\n        ax2.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.6, label=f'Cluster {i+1}')\n    ax2.scatter(kmeans_sklearn.cluster_centers_[:, 0], kmeans_sklearn.cluster_centers_[:, 1], c='black', marker='x', s=100, linewidths=3)\n    ax2.set_title('Sklearn K-Means')\n    ax2.set_xlabel('Bill Length (mm)')\n    ax2.set_ylabel('Flipper Length (mm)')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n\n#load data and run analyses on it\ndf = sns.load_dataset('penguins')\ndf_clean = df.dropna(subset=['bill_length_mm', 'flipper_length_mm'])\nX = df_clean[['bill_length_mm', 'flipper_length_mm']].values\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nlabels, centroids, history = kmeans_custom(X_scaled, k=3, random_state=42)\nplot_kmeans_steps(X_scaled, history, 3, \"K-Means Algorithm Steps\")\n\n\n\n\n\n\n\n\nThe sequence of panels, from “Initial Centroids” through “Step 5,” illustrates the core assign–update loop of the K-Means algorithm applied to bill length and flipper length measurements in the Palmer Penguin dataset. Initially, three centroids are placed arbitrarily among the unlabelled observations. In each subsequent iteration, data points are reallocated to the nearest centroid and the centroids are recomputed as the means of their assigned points. By Steps 3 and 4, cluster membership stabilizes and the migratory adjustments of the centroids diminish. The final panel demonstrates convergence: no observations change cluster assignment, and the centroids reside at the centers of their respective red, green, and blue clusters, confirming the correctness of the custom implementation’s iterative procedure.\n\nwcss, silhouette = analyze_clusters(X_scaled)\n\n\n\n\n\n\n\n\nThe WCSS (Within-Cluster Sum of Squares) curve declines sharply from 247.02 at K=2 to 157.82 at K=3, thereafter exhibiting a pronounced “elbow” that signals diminishing marginal gains in compactness for K&gt;3. In contrast, the average silhouette coefficient attains its maximum of 0.543 at K=2 and steadily decreases thereafter, reaching a nadir near 0.389 at K=6 before a minor uptick at K=7. Together, these diagnostic plots suggest a trade-off between intra-cluster cohesion and inter-cluster separation: two clusters maximize separation, whereas three clusters balance compactness and parsimony most effectively.\n\ncompare_implementations(X_scaled)\n\nprint(\"Results Summary:\")\nfor k, (w, s) in enumerate(zip(wcss, silhouette), 2):\n    print(f\"k={k}: WCSS={w:.2f}, Silhouette={s:.3f}\")\n\n\n\n\n\n\n\n\nResults Summary:\nk=2: WCSS=247.02, Silhouette=0.543\nk=3: WCSS=157.82, Silhouette=0.522\nk=4: WCSS=128.45, Silhouette=0.426\nk=5: WCSS=93.92, Silhouette=0.400\nk=6: WCSS=82.20, Silhouette=0.389\nk=7: WCSS=66.87, Silhouette=0.407\n\n\nOverlaying the custom K-Means solution with scikit-learn’s KMeans for K=3 reveals near‐identical partitionings: point assignments and centroid locations coincide, thereby validating the fidelity of the bespoke algorithm. The accompanying summary table reports WCSS values decreasing monotonically from 247.02 (K=2) to 66.87 (K=7), while silhouette scores decline from 0.543 to 0.389 across the same range. From an analytical standpoint, if the objective is maximal cluster separation, a bi-cluster solution is theoretically optimal; however, when interpretive granularity is paramount, the elbow at K=3 justifies a three-cluster segmentation as the most defensible compromise."
  },
  {
    "objectID": "blog/project4/index.html#a.-k-means",
    "href": "blog/project4/index.html#a.-k-means",
    "title": "Machine Learning from Scratch",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n\n\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this.\n\n#Custom kmeans algorithm function\ndef kmeans_custom(X, k=3, max_iters=100, random_state=42):\n    np.random.seed(random_state)\n    n_samples, n_features = X.shape\n    \n    centroids = X[np.random.choice(n_samples, k, replace=False)]\n    history = [centroids.copy()]\n    \n    for i in range(max_iters):\n        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        \n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        \n        if np.allclose(centroids, new_centroids):\n            break\n            \n        centroids = new_centroids\n        history.append(centroids.copy())\n    \n    return labels, centroids, history\n\n\n#plot kmeans steps!\ndef plot_kmeans_steps(X, history, k, title=\"K-Means Steps\"):\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.ravel()\n    \n    steps_to_show = min(6, len(history))\n    colors = ['red', 'blue', 'green', 'purple', 'orange'][:k]\n    \n    for i in range(steps_to_show):\n        ax = axes[i]\n        \n        if i == 0:\n            ax.scatter(X[:, 0], X[:, 1], alpha=0.6, color='lightgray')\n            ax.scatter(history[i][:, 0], history[i][:, 1], c='red', marker='x', s=100, linewidths=3)\n            ax.set_title(f'Initial Centroids')\n        else:\n            distances = np.sqrt(((X - history[i-1][:, np.newaxis])**2).sum(axis=2))\n            labels = np.argmin(distances, axis=0)\n            \n            for j in range(k):\n                mask = labels == j\n                ax.scatter(X[mask, 0], X[mask, 1], c=colors[j], alpha=0.6, label=f'Cluster {j+1}')\n            \n            ax.scatter(history[i][:, 0], history[i][:, 1], c='black', marker='x', s=100, linewidths=3)\n            ax.set_title(f'Step {i}')\n        \n        ax.set_xlabel('Bill Length (mm)')\n        ax.set_ylabel('Flipper Length (mm)')\n        ax.grid(True, alpha=0.3)\n    \n    for i in range(steps_to_show, 6):\n        axes[i].set_visible(False)\n        \n    plt.tight_layout()\n    plt.suptitle(title, y=1.02, fontsize=16)\n    plt.show()\n\n\n#Helper functions to calculate metrics and compare \n#custom vs scikit learn kmeans outcomes\ndef calculate_wcss(X, labels, centroids):\n    wcss = 0\n    for i in range(len(centroids)):\n        cluster_points = X[labels == i]\n        if len(cluster_points) &gt; 0:\n            wcss += np.sum((cluster_points - centroids[i])**2)\n    return wcss\n\ndef analyze_clusters(X):\n    k_range = range(2, 8)\n    wcss_scores = []\n    silhouette_scores = []\n    \n    for k in k_range:\n        labels, centroids, _ = kmeans_custom(X, k=k, random_state=42)\n        wcss = calculate_wcss(X, labels, centroids)\n        wcss_scores.append(wcss)\n        sil_score = silhouette_score(X, labels)\n        silhouette_scores.append(sil_score)\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    ax1.plot(k_range, wcss_scores, 'bo-', linewidth=2, markersize=8)\n    ax1.set_xlabel('Number of Clusters (k)')\n    ax1.set_ylabel('Within-Cluster Sum of Squares')\n    ax1.set_title('Elbow Method for Optimal k')\n    ax1.grid(True, alpha=0.3)\n    \n    ax2.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n    ax2.set_xlabel('Number of Clusters (k)')\n    ax2.set_ylabel('Silhouette Score')\n    ax2.set_title('Silhouette Analysis')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return wcss_scores, silhouette_scores\n\n\ndef compare_implementations(X):\n    k = 3\n    \n    labels_custom, centroids_custom, _ = kmeans_custom(X, k=k, random_state=42)\n    \n    kmeans_sklearn = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans_sklearn.fit(X)\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    colors = ['red', 'blue', 'green']\n    \n    for i in range(k):\n        mask = labels_custom == i\n        ax1.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.6, label=f'Cluster {i+1}')\n    ax1.scatter(centroids_custom[:, 0], centroids_custom[:, 1], c='black', marker='x', s=100, linewidths=3)\n    ax1.set_title('Custom K-Means')\n    ax1.set_xlabel('Bill Length (mm)')\n    ax1.set_ylabel('Flipper Length (mm)')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    for i in range(k):\n        mask = kmeans_sklearn.labels_ == i\n        ax2.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.6, label=f'Cluster {i+1}')\n    ax2.scatter(kmeans_sklearn.cluster_centers_[:, 0], kmeans_sklearn.cluster_centers_[:, 1], c='black', marker='x', s=100, linewidths=3)\n    ax2.set_title('Sklearn K-Means')\n    ax2.set_xlabel('Bill Length (mm)')\n    ax2.set_ylabel('Flipper Length (mm)')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n\n#load data and run analyses on it\ndf = sns.load_dataset('penguins')\ndf_clean = df.dropna(subset=['bill_length_mm', 'flipper_length_mm'])\nX = df_clean[['bill_length_mm', 'flipper_length_mm']].values\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nlabels, centroids, history = kmeans_custom(X_scaled, k=3, random_state=42)\nplot_kmeans_steps(X_scaled, history, 3, \"K-Means Algorithm Steps\")\n\n\n\n\n\n\n\n\nThe sequence of panels, from “Initial Centroids” through “Step 5,” illustrates the core assign–update loop of the K-Means algorithm applied to bill length and flipper length measurements in the Palmer Penguin dataset. Initially, three centroids are placed arbitrarily among the unlabelled observations. In each subsequent iteration, data points are reallocated to the nearest centroid and the centroids are recomputed as the means of their assigned points. By Steps 3 and 4, cluster membership stabilizes and the migratory adjustments of the centroids diminish. The final panel demonstrates convergence: no observations change cluster assignment, and the centroids reside at the centers of their respective red, green, and blue clusters, confirming the correctness of the custom implementation’s iterative procedure.\n\nwcss, silhouette = analyze_clusters(X_scaled)\n\n\n\n\n\n\n\n\nThe WCSS (Within-Cluster Sum of Squares) curve declines sharply from 247.02 at K=2 to 157.82 at K=3, thereafter exhibiting a pronounced “elbow” that signals diminishing marginal gains in compactness for K&gt;3. In contrast, the average silhouette coefficient attains its maximum of 0.543 at K=2 and steadily decreases thereafter, reaching a nadir near 0.389 at K=6 before a minor uptick at K=7. Together, these diagnostic plots suggest a trade-off between intra-cluster cohesion and inter-cluster separation: two clusters maximize separation, whereas three clusters balance compactness and parsimony most effectively.\n\ncompare_implementations(X_scaled)\n\nprint(\"Results Summary:\")\nfor k, (w, s) in enumerate(zip(wcss, silhouette), 2):\n    print(f\"k={k}: WCSS={w:.2f}, Silhouette={s:.3f}\")\n\n\n\n\n\n\n\n\nResults Summary:\nk=2: WCSS=247.02, Silhouette=0.543\nk=3: WCSS=157.82, Silhouette=0.522\nk=4: WCSS=128.45, Silhouette=0.426\nk=5: WCSS=93.92, Silhouette=0.400\nk=6: WCSS=82.20, Silhouette=0.389\nk=7: WCSS=66.87, Silhouette=0.407\n\n\nOverlaying the custom K-Means solution with scikit-learn’s KMeans for K=3 reveals near‐identical partitionings: point assignments and centroid locations coincide, thereby validating the fidelity of the bespoke algorithm. The accompanying summary table reports WCSS values decreasing monotonically from 247.02 (K=2) to 66.87 (K=7), while silhouette scores decline from 0.543 to 0.389 across the same range. From an analytical standpoint, if the objective is maximal cluster separation, a bi-cluster solution is theoretically optimal; however, when interpretive granularity is paramount, the elbow at K=3 justifies a three-cluster segmentation as the most defensible compromise."
  },
  {
    "objectID": "blog/project4/index.html#a.-k-nearest-neighbors",
    "href": "blog/project4/index.html#a.-k-nearest-neighbors",
    "title": "Machine Learning from Scratch",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\n\n#Set up the data in python\nnp.random.seed(42)\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\nboundary = np.sin(4 * x1) + x1\ny = (x2 &gt; boundary).astype(int)\ndat = pd.DataFrame({\"x1\": x1, \"x2\": x2, \"y\": y})\ndat[\"y\"] = dat[\"y\"].astype(\"category\")\n\nplt.figure(figsize=(10, 6))\ncolors = ['red', 'blue']\nfor i in range(2):\n    mask = dat['y'] == i\n    plt.scatter(dat[mask]['x1'], dat[mask]['x2'], c=colors[i], alpha=0.7, label=f'y={i}')\n\nx1_boundary = np.linspace(-3, 3, 200)\nboundary_line = np.sin(4 * x1_boundary) + x1_boundary\nplt.plot(x1_boundary, boundary_line, 'k--', linewidth=2, label='Boundary')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Training Data with Wiggly Boundary')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nnp.random.seed(123)\nn_test = 100\nx1_test = np.random.uniform(-3, 3, n_test)\nx2_test = np.random.uniform(-3, 3, n_test)\nboundary_test = np.sin(4 * x1_test) + x1_test\ny_test = (x2_test &gt; boundary_test).astype(int)\ntest_dat = pd.DataFrame({\"x1\": x1_test, \"x2\": x2_test, \"y\": y_test})\n\n#Test data\nplt.figure(figsize=(10, 6))\ncolors = ['red', 'blue']\nfor i in range(2):\n    mask = test_dat['y'] == i\n    plt.scatter(test_dat[mask]['x1'], test_dat[mask]['x2'], c=colors[i], alpha=0.7, label=f'y={i}')\n\nx1_boundary = np.linspace(-3, 3, 200)\nboundary_line = np.sin(4 * x1_boundary) + x1_boundary\nplt.plot(x1_boundary, boundary_line, 'k--', linewidth=2, label='Boundary')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Test Data with Wiggly Boundary')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe training‐set visualization depicts 100 synthetic observations colored according to the binary label y y. A sinusoidal decision boundary is overlaid as a dashed curve. Points above the curve are marked in blue and those below in red revealing a highly non‐linear separation and substantial class interleaving near the inflection points. This plot confirms that the dataset embodies a challenging classification problem, one in which local neighborhood structure is critical.\nThe test‐set plot applies the same boundary and coloring scheme to 100 novel points drawn under a different random seed. It illustrates the generalization challenge: although the sinusoidal frontier remains fixed, new observations cluster differently, producing both congruent regions of color and scattered misclassified points when compared to a fitted classifier. This overlay highlights how a KNN rule—especially at small k must adapt to local fluctuations in data density to approximate the true wiggly boundary on unseen data.\n\n#perform kmeans on the setup data\ndef euclidean_distance(x1, x2):\n    return np.sqrt(np.sum((x1 - x2)**2, axis=1))\n\ndef knn_predict(X_train, y_train, X_test, k):\n    predictions = []\n    \n    for test_point in X_test:\n        distances = euclidean_distance(X_train, test_point)\n        k_indices = np.argsort(distances)[:k]\n        k_nearest_labels = y_train[k_indices]\n        prediction = np.bincount(k_nearest_labels).argmax()\n        predictions.append(prediction)\n    \n    return np.array(predictions)\n\nX_train = dat[['x1', 'x2']].values\ny_train = dat['y'].values.astype(int)\nX_test = test_dat[['x1', 'x2']].values\ny_test_true = test_dat['y'].values\n\nk_values = range(1, 31)\naccuracies_custom = []\naccuracies_sklearn = []\n\nfor k in k_values:\n    y_pred_custom = knn_predict(X_train, y_train, X_test, k)\n    accuracy_custom = accuracy_score(y_test_true, y_pred_custom)\n    accuracies_custom.append(accuracy_custom)\n    \n    knn_sklearn = KNeighborsClassifier(n_neighbors=k)\n    knn_sklearn.fit(X_train, y_train)\n    y_pred_sklearn = knn_sklearn.predict(X_test)\n    accuracy_sklearn = accuracy_score(y_test_true, y_pred_sklearn)\n    accuracies_sklearn.append(accuracy_sklearn)\n\nplt.figure(figsize=(12, 6))\nplt.plot(k_values, [acc * 100 for acc in accuracies_custom], 'bo-', label='Custom KNN', linewidth=2, markersize=6)\nplt.plot(k_values, [acc * 100 for acc in accuracies_sklearn], 'ro--', label='Sklearn KNN', linewidth=2, markersize=6)\nplt.xlabel('k (Number of Neighbors)')\nplt.ylabel('Accuracy (%)')\nplt.title('KNN Accuracy vs k')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(range(1, 31, 2))\nplt.show()\n\nbest_k = k_values[np.argmax(accuracies_custom)]\nbest_accuracy = max(accuracies_custom) * 100\n\nprint(f\"Optimal k: {best_k}\")\nprint(f\"Best accuracy: {best_accuracy:.2f}%\")\nprint(f\"Custom vs Sklearn match: {np.allclose(accuracies_custom, accuracies_sklearn)}\")\n\n\n\n\n\n\n\n\nOptimal k: 1\nBest accuracy: 95.00%\nCustom vs Sklearn match: True\n\n\nThe accuracy curve shows that both the hand-coded and scikit-learn KNN classifiers achieve identical performance across all neighbor counts, confirming the correctness of the custom implementation. Classification accuracy peaks at 95 % when k=1, then falls to 91–93 % for small odd values of k, dips further to around 90 % for k≈9 and again near 17, and reaches a minimum of 89 % at k=27. This pattern exemplifies the bias–variance trade-off inherent in KNN: k=1 yields high variance but captures local structure most faithfully (thus highest accuracy on this test set), while larger k values smooth over local fluctuations, increasing bias and reducing accuracy. Consequently, the plot identifies k=1 as the optimal choice for this particular boundary-defined dataset under raw accuracy."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was designed to understand how changes in the perceived “price” of giving affect charitable behavior. Matching grant letters offered donors the opportunity to have their gifts matched by a leadership donor at varying rates, either $1:$1, $2:$1, or $3:$1. This effectively lowers the cost of providing a dollar to the charity. Additionally, the researchers randomized the stated size of the matching grant (e.g., $25,000 or $100,000) and varied the suggested donation amount based on past donor behavior. This allowed for a nuanced investigation of how framing and financial incentives influence both the likelihood of giving and the amount given.\nThe experiment was embedded in a real-world fundraising campaign conducted by a politically liberal nonprofit organization, making it a natural field experiment rather than a lab-based or hypothetical one. The study found that the mere presence of a matching grant substantially increased response rates and average donations. However, increasing the match ratio above 1:1 had no significant additional effect, suggesting that psychological framing, rather than purely economic incentives, plays a key role in donor decision-making. These findings have important implications for fundraising strategy, behavioral economics, and the broader understanding of prosocial behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was designed to understand how changes in the perceived “price” of giving affect charitable behavior. Matching grant letters offered donors the opportunity to have their gifts matched by a leadership donor at varying rates, either $1:$1, $2:$1, or $3:$1. This effectively lowers the cost of providing a dollar to the charity. Additionally, the researchers randomized the stated size of the matching grant (e.g., $25,000 or $100,000) and varied the suggested donation amount based on past donor behavior. This allowed for a nuanced investigation of how framing and financial incentives influence both the likelihood of giving and the amount given.\nThe experiment was embedded in a real-world fundraising campaign conducted by a politically liberal nonprofit organization, making it a natural field experiment rather than a lab-based or hypothetical one. The study found that the mere presence of a matching grant substantially increased response rates and average donations. However, increasing the match ratio above 1:1 had no significant additional effect, suggesting that psychological framing, rather than purely economic incentives, plays a key role in donor decision-making. These findings have important implications for fundraising strategy, behavioral economics, and the broader understanding of prosocial behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.describe()\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# Define variables to test\nvariables = {\n    \"pwhite\": \"Proportion White\",\n    \"pblack\": \"Proportion Black\",\n    \"page18_39\": \"Proportion Age 18-39\",\n    \"ave_hh_sz\": \"Average Household Size\"\n}\n\nresults = []\n\nfor var, label in variables.items():\n    # Subset data\n    treat = df[df[\"treatment\"] == 1][var].dropna()\n    control = df[df[\"treatment\"] == 0][var].dropna()\n\n    # Manual t-test\n    diff = treat.mean() - control.mean()\n    se = np.sqrt(treat.var(ddof=1)/len(treat) + control.var(ddof=1)/len(control))\n    t_stat = diff / se\n    df_total = len(treat) + len(control) - 2\n    p_val_ttest = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=df_total))\n\n    # Regression\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    coef = model.params[\"treatment\"]\n    p_val_reg = model.pvalues[\"treatment\"]\n\n    results.append({\n        \"Variable\": label,\n        \"Diff (Treat - Control)\": diff,\n        \"T-test p-value\": p_val_ttest,\n        \"Regression Coef\": coef,\n        \"Regression p-value\": p_val_reg\n    })\n\npd.DataFrame(results)\n\n\n\n\n\n\n\n\nVariable\nDiff (Treat - Control)\nT-test p-value\nRegression Coef\nRegression p-value\n\n\n\n\n0\nProportion White\n-0.000913\n0.576130\n-0.000913\n0.575308\n\n\n1\nProportion Black\n0.000129\n0.922294\n0.000129\n0.921935\n\n\n2\nProportion Age 18-39\n-0.000124\n0.901123\n-0.000124\n0.901029\n\n\n3\nAverage Household Size\n0.003012\n0.410313\n0.003012\n0.409801\n\n\n\n\n\n\n\nI tested four demographic variables: proportion white, proportion black, proportion aged 18–39, and average household size for balance between treatment and control groups using both two-sample t-tests and simple linear regressions. In all cases, the p-values were well above the 0.05 threshold, indicating no statistically significant differences between groups. Additionally, the t-test differences and regression coefficients matched exactly, confirming consistency between methods. These results suggest that the randomization was successful and that the treatment and control groups are comparable on these baseline characteristics, supporting the validity of subsequent causal inferences."
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n# Calculate donation rates\nresponse_rates = df.groupby('treatment')['gave'].mean()\nlabels = ['Control', 'Treatment']\nvalues = response_rates.values\n\n# Plot\nplt.figure(figsize=(6, 4))\nbars = plt.bar(labels, values, edgecolor='black')\nplt.ylabel('Proportion Donated')\nplt.title('Donation Response Rate by Group')\n\nText(0.5, 1.0, 'Donation Response Rate by Group')\n\n\n\n\n\n\n\n\n\nThe bar charts demonstrate an approximately 5% difference between the proportions of donors in the treatment and control groups. The control group is made up of around 17% of donors while the treatment group is around 22%.\n\n# Subset data\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_control = df[df['treatment'] == 0]['gave']\n\n# Means for reference\nmean_treat = gave_treat.mean()\nmean_control = gave_control.mean()\ndiff = mean_treat - mean_control\n\n# Manual t-test for binary outcome\nn1, n2 = len(gave_treat), len(gave_control)\np1, p2 = mean_treat, mean_control\nvar1 = p1 * (1 - p1)\nvar2 = p2 * (1 - p2)\nse = np.sqrt(var1/n1 + var2/n2)\nt_stat = diff / se\ndf_total = n1 + n2 - 2\np_value_t = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=df_total))\n\n# Regression: gave ~ treatment\nreg = smf.ols('gave ~ treatment', data=df).fit()\nreg_coef = reg.params['treatment']\nreg_pval = reg.pvalues['treatment']\n\n# Display results\n{\n    \"Control Mean\": mean_control,\n    \"Treatment Mean\": mean_treat,\n    \"Difference\": diff,\n    \"T-statistic\": t_stat,\n    \"T-test p-value\": p_value_t,\n    \"Regression Coefficient\": reg_coef,\n    \"Regression p-value\": reg_pval\n}\n\n{'Control Mean': np.float64(0.017858212980164198),\n 'Treatment Mean': np.float64(0.02203856749311295),\n 'Difference': np.float64(0.00418035451294875),\n 'T-statistic': np.float64(3.209540056375026),\n 'T-test p-value': np.float64(0.001330312711992132),\n 'Regression Coefficient': np.float64(0.004180354512949392),\n 'Regression p-value': np.float64(0.0019274025949017077)}\n\n\nI tested whether people who received a fundraising letter with a matching donation offer were more likely to donate compared to those who received a standard letter. A t-test shows that the treatment group had a statistically significantly higher donation rate than the control group. This result is confirmed by a linear regression, where the coefficient on the treatment variable closely matches the observed difference in means and is statistically significant.\nIn practical terms, just mentioning that a matching donation is available makes people more likely to give. Even a small change in framing (e.g., adding a single paragraph about a match) meaningfully influences behavior. This supports the idea that people are motivated not just by the financial impact of their gift, but by social signals like the opportunity to “unlock” funds from another donor. It also reinforces how seemingly minor tweaks in messaging can have outsized effects in fundraising campaigns.\n\ndef summarize_model(model, dep_var=\"y\"):\n    coef = model.params\n    se = model.bse\n    pval = model.pvalues\n    zval = model.tvalues  # also t-values for OLS\n    ci = model.conf_int()\n    n = int(model.nobs)\n\n    is_probit = hasattr(model, 'prsquared')  # True for Probit/Logit, not for OLS\n\n    if is_probit:\n        print(f\"### Probit Regression: {dep_var} ~ [predictors]\")\n        print(f\"**Sample size:** {n}\")\n        print(f\"**Pseudo R-squared:** {model.prsquared:.4f}\")\n        print(f\"**Log-likelihood:** {model.llf:.1f}\")\n        print(f\"**Model significance (LLR p-value):** {model.llr_pvalue:.4f}\")\n    else:\n        print(f\"### OLS Regression: {dep_var} ~ [predictors]\")\n        print(f\"**Sample size:** {n}\")\n        print(f\"**R-squared:** {model.rsquared:.4f}\")\n\n    print(\"\\n| Variable   | Coefficient | Std. Error | z/t-value | p-value | 95% CI            |\")\n    print(\"|------------|-------------|------------|-----------|---------|-------------------|\")\n\n    for var in coef.index:\n        ci_lower = ci.loc[var, 0]\n        ci_upper = ci.loc[var, 1]\n        print(f\"| {var:&lt;10} | {coef[var]:&gt;11.4f} | {se[var]:&gt;10.4f} | {zval[var]:&gt;9.2f} | {pval[var]:&gt;7.4f} | [{ci_lower:.3f}, {ci_upper:.3f}] |\")\n\n\n# Probit regression\nprobit_model = smf.probit('gave ~ treatment', data=df).fit()\nsummarize_model(probit_model, 'gave')\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n### Probit Regression: gave ~ [predictors]\n**Sample size:** 50083\n**Pseudo R-squared:** 0.0010\n**Log-likelihood:** -5030.5\n**Model significance (LLR p-value):** 0.0017\n\n| Variable   | Coefficient | Std. Error | z/t-value | p-value | 95% CI            |\n|------------|-------------|------------|-----------|---------|-------------------|\n| Intercept  |     -2.1001 |     0.0233 |    -90.07 |  0.0000 | [-2.146, -2.054] |\n| treatment  |      0.0868 |     0.0279 |      3.11 |  0.0019 | [0.032, 0.141] |\n\n\nI estimated a probit regression to examine whether assignment to the treatment group receiving a matching donation offer increased the likelihood that an individual made a charitable donation. The dependent variable was “gave”, a binary indicator equal to 1 if any donation was made. The key explanatory variable was treatment, equal to 1 for individuals who received a matching offer.\nThe probit model results show a positive and statistically significant coefficient of 0.0868 on the treatment variable (p = 0.002). This matches the findings reported in Table 3, Column 1 of Karlan & List (2007), where the coefficient on treatment is also approximately 0.087 and highly significant.\nThis result confirms that simply including a matching grant offer in a fundraising letter increases the probability of giving. The significance of the coefficient suggests that the effect is not due to chance. While the magnitude of the effect is modest in absolute terms, its consistency across t-tests, OLS, and probit models strengthens the evidence that framing a donation as being matched can meaningfully influence donor behavior.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Limit to only people who received a treatment letter (match offered)\ntreated = df[df['treatment'] == 1]\n\n# Get donation responses by match ratio\ngave_1to1 = treated[treated['ratio'] == 1]['gave']\ngave_2to1 = treated[treated['ratio'] == 2]['gave']\ngave_3to1 = treated[treated['ratio'] == 3]['gave']\n\n# Calculate means for each group\nmean_1 = gave_1to1.mean()\nmean_2 = gave_2to1.mean()\nmean_3 = gave_3to1.mean()\n\n# Run t-tests\nt_2_vs_1 = ttest_ind(gave_2to1, gave_1to1)\nt_3_vs_2 = ttest_ind(gave_3to1, gave_2to1)\nt_3_vs_1 = ttest_ind(gave_3to1,gave_1to1)\n\n{\n    \"1:1 Match Rate\": mean_1,\n    \"2:1 Match Rate\": mean_2,\n    \"3:1 Match Rate\": mean_3,\n    \"p-value: 2:1 vs 1:1\": t_2_vs_1.pvalue,\n    \"p-value: 3:1 vs 2:1\": t_3_vs_2.pvalue,\n    \"p-value: 3:1 vs 1:1\": t_3_vs_1.pvalue\n}\n\n{'1:1 Match Rate': np.float64(0.020749124225276205),\n '2:1 Match Rate': np.float64(0.0226333752469912),\n '3:1 Match Rate': np.float64(0.022733399227244138),\n 'p-value: 2:1 vs 1:1': np.float64(0.33453168549723933),\n 'p-value: 3:1 vs 2:1': np.float64(0.9600305283739325),\n 'p-value: 3:1 vs 1:1': np.float64(0.31010466370866724)}\n\n\nTo assess whether larger match ratios increase donation rates, I performed a series of t-tests comparing response rates among individuals assigned to 1:1, 2:1, and 3:1 match treatment groups. The donation rate was approximately 2.07% for the 1:1 group, 2.26% for 2:1, and 2.27% for 3:1. Although there is a small increase in the average response rate as the match ratio increases, none of the differences are statistically significant: the p-value comparing 2:1 to 1:1 is 0.335, and for 3:1 vs. 2:1 it is 0.960. Even the largest comparison (3:1 vs. 1:1) yields a p-value of 0.310, far above the conventional 0.05 threshold for significance.\nThese results support the authors’ “figures suggest” comment on page 8 of the paper: while the raw response rates rise slightly with higher match ratios, the increases are not statistically meaningful. This suggests that once a match is offered, increasing the size of that match even up to 3:1 does not further motivate donors to give\n\ntreated = df[df['treatment'] == 1].copy()\n\n# Create dummy variables \ntreated['ratio1'] = (treated['ratio'] == 1).astype(int)\ntreated['ratio2'] = (treated['ratio'] == 2).astype(int)\ntreated['ratio3'] = (treated['ratio'] == 3).astype(int)\n\n# Regression: gave ~ ratio2 + ratio3 \nratio_reg = smf.ols('gave ~ ratio2 + ratio3', data=treated).fit()\n\nsummarize_model(ratio_reg, 'gave')\n\n### OLS Regression: gave ~ [predictors]\n**Sample size:** 33396\n**R-squared:** 0.0000\n\n| Variable   | Coefficient | Std. Error | z/t-value | p-value | 95% CI            |\n|------------|-------------|------------|-----------|---------|-------------------|\n| Intercept  |      0.0207 |     0.0014 |     14.91 |  0.0000 | [0.018, 0.023] |\n| ratio2     |      0.0019 |     0.0020 |      0.96 |  0.3383 | [-0.002, 0.006] |\n| ratio3     |      0.0020 |     0.0020 |      1.01 |  0.3133 | [-0.002, 0.006] |\n\n\nTo assess whether the size of the match ratio influenced donation behavior, I regressed the binary outcome variable gave on dummy variables for 2:1 and 3:1 match ratios, using the 1:1 match as the baseline. The results show that neither the 2:1 nor 3:1 match ratios had a statistically significant effect on the likelihood of donating compared to the 1:1 match. The coefficient for ratio2 was 0.0019 (p = 0.338), and for ratio3 it was 0.0020 (p = 0.313), both with confidence intervals that include zero. The intercept, representing the baseline 1:1 match group, was 0.0207, consistent with earlier descriptive statistics. These findings confirm that increasing the size of the match beyond 1:1 does not meaningfully increase donation rates, reinforcing the paper’s conclusion that the presence of a match offer matters more than its size.\n\nmean_1 = gave_1to1.mean()\nmean_2 = gave_2to1.mean()\nmean_3 = gave_3to1.mean()\n\n# Raw differences\nraw_diff_2_vs_1 = mean_2 - mean_1\nraw_diff_3_vs_2 = mean_3 - mean_2\n\n# From regression results\ncoef_2 = ratio_reg.params['ratio2']\ncoef_3 = ratio_reg.params['ratio3']\n\n# Fitted differences (1:1 is baseline = intercept)\nfitted_diff_2_vs_1 = coef_2\nfitted_diff_3_vs_2 = coef_3 - coef_2\n\n{\n    \"Raw Difference (2:1 - 1:1)\": raw_diff_2_vs_1,\n    \"Raw Difference (3:1 - 2:1)\": raw_diff_3_vs_2,\n    \"Fitted Difference (2:1 - 1:1)\": fitted_diff_2_vs_1,\n    \"Fitted Difference (3:1 - 2:1)\": fitted_diff_3_vs_2\n}\n\n{'Raw Difference (2:1 - 1:1)': np.float64(0.0018842510217149944),\n 'Raw Difference (3:1 - 2:1)': np.float64(0.00010002398025293902),\n 'Fitted Difference (2:1 - 1:1)': np.float64(0.0018842510217148354),\n 'Fitted Difference (3:1 - 2:1)': np.float64(0.00010002398025296178)}\n\n\nTo further assess the effect of increasing the match ratio on donation rates, I calculated both raw and regression-based differences in response rates. The raw data show a donation rate of 2.07% for the 1:1 group, 2.26% for the 2:1 group, and 2.27% for the 3:1 group. The difference between 2:1 and 1:1 is approximately 0.0019, while the difference between 3:1 and 2:1 is just 0.0001. These match almost exactly the fitted differences obtained from the regression coefficients (0.0019 and 0.0001, respectively). The consistency between the raw and model-based estimates reinforces the conclusion that larger match ratios do not meaningfully increase the likelihood of giving. The small and statistically insignificant differences confirm that the presence of a match matters, but its generosity does not seem to influence donor behavior beyond that initial effect.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# Bivariate regression: total amount donated ~ treatment\namount_reg = smf.ols('amount ~ treatment', data=df).fit()\nsummarize_model(amount_reg, 'amount')\n\n### OLS Regression: amount ~ [predictors]\n**Sample size:** 50083\n**R-squared:** 0.0001\n\n| Variable   | Coefficient | Std. Error | z/t-value | p-value | 95% CI            |\n|------------|-------------|------------|-----------|---------|-------------------|\n| Intercept  |      0.8133 |     0.0674 |     12.06 |  0.0000 | [0.681, 0.945] |\n| treatment  |      0.1536 |     0.0826 |      1.86 |  0.0628 | [-0.008, 0.315] |\n\n\nTo assess whether offering a matching donation influenced not just the likelihood of giving but also the amount donated, I regressed the dollar amount of contributions on the treatment assignment indicator. The results show that the average donation in the control group was $0.81, as reflected in the intercept. Individuals in the treatment group gave $0.15 more on average, though this difference is not statistically significant at the 5% level (p = 0.063). While the coefficient is positive and suggestive of a modest increase in donation size, the lack of statistical significance means we cannot confidently conclude that the treatment had a reliable effect on donation amounts. This suggests that the primary effect of the matching offer was to increase participation, rather than the average amount given per donor.\n\ndonors = df[df['gave']&gt;0]\n\namount_reg2 = smf.ols('amount ~ treatment', data=donors).fit()\namount_reg2.summary()\n\nsummarize_model(amount_reg2, 'amount')\n\n### OLS Regression: amount ~ [predictors]\n**Sample size:** 1034\n**R-squared:** 0.0003\n\n| Variable   | Coefficient | Std. Error | z/t-value | p-value | 95% CI            |\n|------------|-------------|------------|-----------|---------|-------------------|\n| Intercept  |     45.5403 |     2.4234 |     18.79 |  0.0000 | [40.785, 50.296] |\n| treatment  |     -1.6684 |     2.8724 |     -0.58 |  0.5615 | [-7.305, 3.968] |\n\n\nTo understand whether the treatment influenced how much people donated, conditional on having donated, I restricted the dataset to individuals who made a donation (gave &gt; 0) and regressed amount on the treatment indicator. The results show that the average donation amount in the control group was approximately $45.54, while the treatment group gave $1.67 less on average, according to the regression coefficient. However, this difference is not statistically significant (p = 0.561), and the 95% confidence interval includes zero (−7.31 to 3.97). This indicates that among those who did choose to donate, being offered a matching grant did not significantly affect the donation amount.\n\n# Separate treatment and control donors\ntreat_donors = donors[donors['treatment'] == 1]['amount']\ncontrol_donors = donors[donors['treatment'] == 0]['amount']\n\n# Calculate means\nmean_treat = treat_donors.mean()\nmean_control = control_donors.mean()\n\n# Plot: Control Group\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.hist(control_donors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\nplt.axvline(mean_control, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean_control:.2f}')\nplt.title(\"Control Group Donations\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\n\n# Plot: Treatment Group\nplt.subplot(1, 2, 2)\nplt.hist(treat_donors, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.axvline(mean_treat, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean_treat:.2f}')\nplt.title(\"Treatment Group Donations\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese histograms show the distribution of donation amounts among individuals who gave, separated by treatment and control groups. While both distributions are right-skewed, the average donation is slightly lower in the treatment group ($43.87) than in the control group ($45.54), indicating that the match offer did not increase conditional donation amounts."
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate donation behavior: Bernoulli draws\n# 100,000 from control group, p = 0.018\n# 10,000 from treatment group, p = 0.022\nnp.random.seed(42)\ncontrol_draws = np.random.binomial(1, 0.018, 100000)\ntreat_draws = np.random.binomial(1, 0.022, 10000)\n\n# Calculate pointwise differences between treatment and control draws\ndiffs = treat_draws - control_draws[:10000]  # align sizes\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot cumulative average\nplt.figure(figsize=(8, 4))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences')\nplt.axhline(0.004, color='red', linestyle='--', label='True Difference (0.022 - 0.018)')\nplt.title(\"Law of Large Numbers: Cumulative Average of Simulated Differences\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot illustrates the Law of Large Numbers by showing how the cumulative average of simulated differences in donation behavior between treatment and control groups stabilizes as the sample size increases. Initially, the cumulative average fluctuates widely due to random variation in small samples, but as more observations accumulate, the average converges toward the true difference in population means: 0.004 (marked by the red dashed line). This visual evidence confirms that with a large enough sample, the observed difference in donation rates between treatment and control becomes a reliable estimate of the actual treatment effect.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulation setup\nnp.random.seed(42)\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\n# Function to simulate one sample difference\ndef simulate_diffs(n, p1, p2, reps=1000):\n    diffs = []\n    for _ in range(reps):\n        c = np.random.binomial(1, p1, n)\n        t = np.random.binomial(1, p2, n)\n        diffs.append(np.mean(t) - np.mean(c))\n    return np.array(diffs)\n\n# Generate and plot\nplt.figure(figsize=(12, 10))\n\nfor i, n in enumerate(sample_sizes, 1):\n    diffs = simulate_diffs(n, p_control, p_treatment)\n    plt.subplot(2, 2, i)\n    plt.hist(diffs, bins=30, edgecolor='black', alpha=0.7, density=True)\n    plt.axvline(0, color='black', linestyle='--', linewidth=1, label='Zero')\n    plt.axvline(0.004, color='red', linestyle='--', linewidth=2, label='True Diff = 0.004')\n    plt.title(f\"Sample Size = {n}\")\n    plt.xlabel(\"Difference in Means\")\n    plt.ylabel(\"Density\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese four histograms illustrate the Central Limit Theorem by showing the sampling distribution of the difference in means between treatment and control groups across varying sample sizes (50, 200, 500, and 1000). At smaller sample sizes (e.g., 50), the distribution is wide and irregular, and zero frequently appears near the center—indicating that we might observe no difference simply due to sampling variability. As the sample size increases, the distribution becomes more concentrated and symmetric, and the center of the distribution shifts closer to the true difference of 0.004 (indicated by the red dashed line). By the time we reach a sample size of 1000, the sampling distribution is tightly centered around the true effect, and zero lies in the tail rather than the center. This pattern highlights how increasing sample size leads to more precise and reliable estimates, reducing the likelihood that random chance obscures true treatment effects"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kai’s Stern",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "blog/project3/mnl.html",
    "href": "blog/project3/mnl.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "Suppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project3/mnl.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project3/mnl.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "Suppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project3/mnl.html#simulate-conjoint-data",
    "href": "blog/project3/mnl.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data."
  },
  {
    "objectID": "blog/project3/mnl.html#preparing-the-data-for-estimation",
    "href": "blog/project3/mnl.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n\n\n\nconjoint_data = pd.read_csv('/Users/kaistern/Documents/MGTA_Data/MGTA_Marketing_Analytics/quarto_website/blog/project3/conjoint_data.csv')\n\n\nconjoint_data['is_netflix'] = (conjoint_data['brand'] == 'N').astype(int)\nconjoint_data['is_prime'] = (conjoint_data['brand'] == 'P').astype(int)\nconjoint_data['has_ads'] = (conjoint_data['ad'] == 'Yes').astype(int)\n\n\nX = conjoint_data[['is_netflix', 'is_prime', 'has_ads', 'price']].values\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\ny = conjoint_data['choice'].values\n\nconjoint_data['task_id'] = conjoint_data['resp'] * 100 + conjoint_data['task']"
  },
  {
    "objectID": "blog/project3/mnl.html#estimation-via-maximum-likelihood",
    "href": "blog/project3/mnl.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\ntask_ids = conjoint_data['task_id'].values\n\ndef mnl_log_likelihood(beta, X, y, task_ids):\n    beta = np.asarray(beta)\n    Xb = X @ beta\n    ll = 0.0\n    for task in np.unique(task_ids):\n        idx = task_ids == task\n        Xb_task = Xb[idx]\n        y_task = y[idx]\n        P = np.exp(Xb_task - np.max(Xb_task))  \n        P /= P.sum()\n        ll += np.sum(y_task * np.log(P))\n    return -ll  \n\n\n# Initial guess\nbeta_init = np.zeros(X.shape[1])\n\n# Run optimizer with Hessian\nres = minimize(mnl_log_likelihood, beta_init, args=(X, y, task_ids), method='BFGS', options={'disp': True})\n\n# Extract estimated parameters\nbeta_hat = res.x\nlog_likelihood = -res.fun\n\n# Standard errors from inverse Hessian\nhessian_inv = res.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# 95% confidence intervals\nz_score = norm.ppf(0.975)  # ≈ 1.96\nconf_int_lower = beta_hat - z_score * standard_errors\nconf_int_upper = beta_hat + z_score * standard_errors\n\n# Display results\nfor i, name in enumerate(['netflix', 'prime', 'ads', 'price']):\n    print(f\"{name:&gt;7}: β = {beta_hat[i]: .4f}, SE = {standard_errors[i]: .4f}, 95% CI = [{conf_int_lower[i]:.4f}, {conf_int_upper[i]:.4f}]\")\n\nprint(\"\\nLog-likelihood:\", log_likelihood)\n\n         Current function value: 879.855368\n         Iterations: 13\n         Function evaluations: 317\n         Gradient evaluations: 62\nnetflix: β =  0.4454, SE =  0.0315, 95% CI = [0.3837, 0.5071]\n  prime: β =  0.2372, SE =  0.0667, 95% CI = [0.1066, 0.3679]\n    ads: β = -0.3658, SE =  0.1053, 95% CI = [-0.5722, -0.1595]\n  price: β = -0.7937, SE =  0.1331, 95% CI = [-1.0546, -0.5328]\n\nLog-likelihood: -879.8553682672009\n\n\n/Users/kaistern/Documents/MGTA_Data/MGTA_Marketing_Analytics/quarto_website/.venv/lib/python3.13/site-packages/scipy/optimize/_minimize.py:733: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n\n\nThis output summarizes the results of fitting a multinomial logit model using maximum likelihood estimation. The model converged after 13 iterations, with a final log-likelihood of -879.86. The estimates show that Netflix has the highest positive effect on utility (β = 0.4454), followed by Prime (β = 0.2372), indicating a general preference for Netflix over Prime. The ads coefficient is negative (β = -0.3658), suggesting that the presence of ads reduces utility, while the price coefficient is strongly negative (β = -0.7937), confirming that higher prices make a service less attractive. All coefficients are statistically significant at the 95% level, as their confidence intervals do not include zero. The warning at the end suggests that there may have been numerical issues during optimization, but the solution still appears interpretable and reasonable."
  },
  {
    "objectID": "blog/project3/mnl.html#estimation-via-bayesian-methods",
    "href": "blog/project3/mnl.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\ndef log_prior(beta):\n    \n    lp = np.sum(-0.5 * (beta[:3] ** 2) / 5 - 0.5 * np.log(2 * np.pi * 5))\n    lp += -0.5 * beta[3] ** 2 - 0.5 * np.log(2 * np.pi)\n    return lp\n\n\ndef log_posterior(beta, X, y, task_ids):\n    log_lik = -1 * mnl_log_likelihood(beta, X, y, task_ids)  # flip sign\n    return log_lik + log_prior(beta)\n\n\ndef metropolis_hastings(log_post_fn, X, y, task_ids, n_steps=11000,\n                        proposal_sds=[0.05, 0.05, 0.05, 0.005]):\n    beta_current = np.zeros(4)\n    log_post_current = log_post_fn(beta_current, X, y, task_ids)\n    samples = []\n    acceptances = 0\n\n    for step in range(n_steps):\n        proposal = beta_current + np.random.normal(0, proposal_sds)\n        log_post_proposal = log_post_fn(proposal, X, y, task_ids)\n\n        log_accept_ratio = log_post_proposal - log_post_current\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            beta_current = proposal\n            log_post_current = log_post_proposal\n            acceptances += 1\n\n        samples.append(beta_current.copy())\n\n    print(f\"Acceptance rate: {acceptances / n_steps:.3f}\")\n    return np.array(samples)\n\n\nsamples = metropolis_hastings(log_posterior, X, y, task_ids)\nposterior_samples = samples[1000:]  # discard first 1,000 \n\nAcceptance rate: 0.406\n\n\n\nfor i in range(4):\n    param_samples = posterior_samples[:, i]\n    mean = np.mean(param_samples)\n    std = np.std(param_samples)\n    ci_lower = np.percentile(param_samples, 2.5)\n    ci_upper = np.percentile(param_samples, 97.5)\n    print(f\"Beta {i}: mean={mean:.3f}, std={std:.3f}, 95% CI=({ci_lower:.3f}, {ci_upper:.3f})\")\n\nBeta 0: mean=0.445, std=0.055, 95% CI=(0.341, 0.553)\nBeta 1: mean=0.239, std=0.055, 95% CI=(0.131, 0.349)\nBeta 2: mean=-0.364, std=0.045, 95% CI=(-0.453, -0.276)\nBeta 3: mean=-0.777, std=0.044, 95% CI=(-0.874, -0.692)\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(posterior_samples[:, 3])\nplt.title(\"Trace Plot: Beta 3 (Price)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Value\")\nplt.show()\n\nplt.hist(posterior_samples[:, 3], bins=30, density=True)\nplt.title(\"Posterior Histogram: Beta 3 (Price)\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nposterior_samples = samples[1000:]  # discard first 1,000 \n\nsummary = []\nfor i in range(4):\n    param_samples = posterior_samples[:, i]\n    mean = np.mean(param_samples)\n    std = np.std(param_samples)\n    ci_lower = np.percentile(param_samples, 2.5)\n    ci_upper = np.percentile(param_samples, 97.5)\n    summary.append((mean, std, ci_lower, ci_upper))\n\n\nfrom scipy.optimize import minimize\n\nresult = minimize(mnl_log_likelihood, x0=np.zeros(4), args=(X, y, task_ids))\nmle_beta = result.x\n\n\nprint(\"Parameter  |  Posterior Mean  |  Std Dev  |  95% CI                 |  MLE Estimate\")\nprint(\"-----------|------------------|-----------|--------------------------|----------------\")\nfor i in range(4):\n    post_mean, post_std, lower, upper = summary[i]\n    print(f\"Beta {i:&lt;5} | {post_mean:&gt;16.3f} | {post_std:&gt;9.3f} | ({lower:&gt;6.3f}, {upper:&gt;6.3f}) | {mle_beta[i]:&gt;13.3f}\")\n\nParameter  |  Posterior Mean  |  Std Dev  |  95% CI                 |  MLE Estimate\n-----------|------------------|-----------|--------------------------|----------------\nBeta 0     |            0.445 |     0.055 | ( 0.341,  0.553) |         0.445\nBeta 1     |            0.239 |     0.055 | ( 0.131,  0.349) |         0.237\nBeta 2     |           -0.364 |     0.045 | (-0.453, -0.276) |        -0.366\nBeta 3     |           -0.777 |     0.044 | (-0.874, -0.692) |        -0.794"
  },
  {
    "objectID": "blog/project3/mnl.html#discussion",
    "href": "blog/project3/mnl.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf the data were not simulated but instead came from actual respondents, the parameter estimates reflect real-world preferences rather than values defined by a simulation. In this case, we observe that the estimated coefficients provide insight into how different attributes, such as brand and price, influence consumer choices. For example, if the estimate for \\(\\beta_\\text{Netflix}\\) is greater than that for \\(\\beta_\\text{Prime}\\), it indicates that respondents, on average, prefer Netflix over Prime Video, all else being equal. This suggests that the Netflix brand is associated with higher perceived utility or desirability in the choice context. Additionally, a negative coefficient for price, \\(\\beta_\\text{price}\\), makes intuitive sense: it means that as the price of a streaming service increases, its attractiveness decreases, which aligns with standard economic theory that higher prices tend to reduce demand. Overall, the signs and magnitudes of the estimates provide a coherent and interpretable picture of how people make trade-offs between different features when choosing among alternatives.\nTo simulate data from and estimate the parameters of a multi-level model, you need to move beyond the assumption that all individuals share the same preferences. Instead, you allow each individual to have their own set of utility coefficients, which are drawn from a population-level distribution. Typically, this is modeled as a multivariate normal distribution, where each respondent’s coefficients (betas) are drawn from a common distribution with a mean vector, representing average preferences across the population, and a covariance matrix (capturing the variability and correlation of preferences across individuals). To simulate data, you would first sample individual-level betas from this distribution, and then simulate choices based on these betas and the design matrix. To estimate such a model, you would need to use hierarchical Bayesian methods to infer both the individual-level parameters and the group-level hyperparameters. This structure captures heterogeneity in preferences and better reflects how real people vary in their responses, making it the preferred approach for analyzing real-world conjoint data."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\nimport statsmodels.api as sm\nimport math\nfrom scipy.special import gammaln\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n\n\n\nblueprint = pd.read_csv(\"/Users/kaistern/Documents/MGTA_Data/MGTA_Marketing_Analytics/quarto_website/blog/project2/blueprinty.csv\")\n\nairbnb = pd.read_csv(\"/Users/kaistern/Documents/MGTA_Data/MGTA_Marketing_Analytics/quarto_website/blog/project2/airbnb.csv\")\n\n\ncusts = blueprint[blueprint['iscustomer']==1]\nnon_custs = blueprint[blueprint['iscustomer']==0]\n\ncust_mean = custs['patents'].mean()\nnon_cust_mean = non_custs['patents'].mean()\n\nprint(f'The mean number of patents for customers is {cust_mean} while the mean for non-patients is {non_cust_mean}')\n\nThe mean number of patents for customers is 4.133056133056133 while the mean for non-patients is 3.4730127576054954\n\n\n\nplt.figure(figsize=(8, 5))\nplt.hist(custs['patents'], bins=30, color='skyblue', edgecolor='black')\nplt.title(\"Patent Distribution - Customers\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(8, 5))\nplt.hist(non_custs['patents'], bins=30, color='salmon', edgecolor='black')\nplt.title(\"Patent Distribution - Non-Customers\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe histograms show that both customers and non-customers have right-skewed distributions of patent counts, with most values concentrated below 6. Customers peak around 4 patents, while non-customers peak slightly lower, around 2–3. The mean number of patents is higher for customers (4.13) than for non-customers (3.47), suggesting that customers tend to be slightly more innovation-active. Despite a larger sample size among non-customers, the customer group shows a modest upward shift in patent ownership.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nplt.figure(figsize=(8, 5))\nsns.countplot(data=blueprint, x='region', hue='iscustomer')\nplt.title(\"Customer vs Non-Customer Counts by Region\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe regional distribution of customers and non-customers is clearly imbalanced. The Northeast region stands out with more customers than non-customers, while all other regions have significantly fewer customers. This suggests that customer status is not randomly distributed across regions and regional factors may influence Blueprinty selection.\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=blueprint, x='iscustomer', y='age')\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Customer Status (0 = Non-Customer, 1 = Customer)\")\nplt.ylabel(\"Age\")\nplt.xticks([0, 1], ['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe boxplot shows that customers tend to be slightly older than non-customers on average, with a modest upward shift in median age. Both groups have similar spreads and ranges, though customers show slightly higher values at the upper quartile. This indicates age may play a small but non-negligible role in customer selection.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for independent observations ( Y_1, , Y_n (_i) ) is:\n\\[\n\\mathcal{L}(\\beta) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!} = \\prod_{i=1}^n \\frac{e^{-e^{X_i^\\top \\beta}} \\left(e^{X_i^\\top \\beta}\\right)^{Y_i}}{Y_i!}\n\\]\nTaking the log of the likelihood gives the log-likelihood:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ Y_i X_i^\\top \\beta - e^{X_i^\\top \\beta} - \\log(Y_i!) \\right]\n\\]\nI implement this function below in python.\n\ndef log_factorial(y):\n    return np.array([np.sum(np.log(np.arange(1, int(yi)+1))) if yi &gt; 0 else 0 for yi in y])\n\ndef poisson_log_likelihood(lambd, y):\n    y = np.asarray(y)\n    n = len(y)\n\n    log_fact = log_factorial(y)\n    log_likelihood = -n * lambd + np.sum(y * np.log(lambd)) - np.sum(log_fact)\n    return log_likelihood\n\n\nY = blueprint['patents'].values  \nlambda_vals = np.linspace(0.1, 10, 200)  \n\n\nlog_likelihoods = [poisson_log_likelihood(lam, Y) for lam in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_likelihoods, label='Log-Likelihood')\nplt.axvline(lambda_vals[np.argmax(log_likelihoods)], color='red', linestyle='--', label='MLE λ')\nplt.title(\"Poisson Log-Likelihood as a Function of λ\")\nplt.xlabel(\"λ (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe log-likelihood curve rises sharply at low values of λ, peaks around the sample mean of patent counts (approximately 4), and then gradually declines reflecting the typical shape of a Poisson likelihood. The red dashed line marks the maximum likelihood estimate for λ, which corresponds to the value that best explains the observed data under the Poisson model.\n\nY = blueprint['patents'].values\n\nobjective = lambda lam: -poisson_log_likelihood(lam, Y)\n\nresult = minimize_scalar(objective, bounds=(0.01, 10), method='bounded')\n\nmle_lambda = result.x\nprint(f\"MLE for λ: {mle_lambda:.4f}\")\n\nMLE for λ: 3.6847\n\n\nThe MLE is 3.6847 which is aligns with the graphical output above and makes sense given our understanding of the data up to this point.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\ndef poisson_regression_log_likelihood(beta, X, y):\n    beta = np.atleast_1d(beta)\n    X = np.asarray(X)\n    y = np.asarray(y)\n\n    eta = X @ beta\n    lambdas = np.array([math.exp(val) for val in eta])\n\n    log_fact = gammaln(y + 1)  # log(y!) using gamma function\n\n    return np.sum(y * eta - lambdas - log_fact)\n\n\ndef poisson_loglik(beta, X, y):\n    eta = X @ beta\n    return sum(y[i]*eta[i] - math.exp(min(eta[i], 700)) - gammaln(y[i]+1) for i in range(len(y)))\n\n\ndef fit_poisson(df, y_col, features):\n    X = sm.add_constant(df[features]).values\n    y = df[y_col].values\n    neg_ll = lambda b: -poisson_loglik(b, X, y)\n    beta0 = np.zeros(X.shape[1])\n\n    res = minimize(neg_ll, beta0, method='Nelder-Mead')\n    if not res.success:\n        res = minimize(neg_ll, beta0, method='BFGS')\n\n    beta_hat = res.x\n    n = len(beta_hat)\n    eps = 1e-5\n    hess = np.zeros((n, n))\n\n    for i in range(n):\n        for j in range(i, n):\n            di, dj = np.zeros(n), np.zeros(n)\n            di[i], dj[j] = eps, eps\n            f = neg_ll(beta_hat)\n            f1 = neg_ll(beta_hat + di)\n            f2 = neg_ll(beta_hat + dj)\n            f12 = neg_ll(beta_hat + di + dj)\n            hess[i, j] = (f12 - f1 - f2 + f) / eps**2\n            hess[j, i] = hess[i, j]\n\n    try:\n        se = np.sqrt(np.diag(np.linalg.inv(hess)))\n    except:\n        se = np.full(n, np.nan)\n\n    summary = pd.DataFrame({'Coefficient': beta_hat, 'Std. Error': se}, index=sm.add_constant(df[features]).columns)\n\n    return {\n        'coefficients': beta_hat,\n        'std_errors': se,\n        'summary': summary,\n        'convergence': res.success,\n        'log_likelihood': -res.fun\n    }\n\n\ndf = blueprint.copy()\ndf['age_squared'] = df['age'] ** 2\ndf = pd.get_dummies(df, columns=['region'], drop_first=True)\nfeatures = ['age', 'age_squared', 'iscustomer'] + [col for col in df.columns if col.startswith('region_')]\n\nresult = fit_poisson(df, 'patents', features)\nprint(result['summary'])\n\n                  Coefficient  Std. Error\nconst               -0.329507    0.179246\nage                  0.137006    0.013514\nage_squared         -0.002764    0.000249\niscustomer           0.214490    0.030893\nregion_Northeast    -0.013539    0.043277\nregion_Northwest    -0.053201    0.053562\nregion_South         0.022149    0.052401\nregion_Southwest     0.042996    0.046625\n\n\n\nfeatures = ['age', 'age_squared', 'iscustomer'] + [col for col in df.columns if col.startswith('region_')]\nX = sm.add_constant(df[features]).astype(float)  \ny = df['patents'].astype(float)                  \n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nresult_glm = model.fit()\n\nprint(result_glm.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 09 Jun 2025   Deviance:                       2143.3\nTime:                        12:08:03   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst               -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage                  0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared         -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer           0.2076      0.031      6.719      0.000       0.147       0.268\nregion_Northeast     0.0292      0.044      0.669      0.504      -0.056       0.115\nregion_Northwest    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nregion_South         0.0566      0.053      1.074      0.283      -0.047       0.160\nregion_Southwest     0.0506      0.047      1.072      0.284      -0.042       0.143\n====================================================================================\n\n\nThe Poisson regression results suggest that age, age squared, and iscustomer status are significant predictors of the number of patents held, while regional differences are not statistically significant. Specifically, age has a positive coefficient (0.149), but the negative coefficient on age_squared (−0.003) indicates a concave relationship: the expected patent count increases with age at a decreasing rate. Being a customer is associated with a ~21% increase in expected patents (exp(0.208) ≈ 1.23), and this effect is highly significant. The intercept is also significant, while all regional indicators have large p-values (p &gt; 0.28), suggesting no strong regional differences in patent counts after accounting for other variables. The pseudo R-squared (0.136) indicates a modest fit.\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\ny_pred_0 = result_glm.predict(X_0)\ny_pred_1 = result_glm.predict(X_1)\n\ndiff = y_pred_1 - y_pred_0\naverage_effect = diff.mean()\n\nprint(f\"Average marginal effect of Blueprinty's software on expected patent count: {average_effect:.3f}\")\n\nAverage marginal effect of Blueprinty's software on expected patent count: 0.793\n\n\nOn average, being a Blueprinty customer is associated with 0.793 more expected patents per firm, holding all other characteristics constant. This suggests a substantive positive impact of the software on firms’ innovation output, assuming the model is well-specified and the number of patents is a good outcome measure."
  },
  {
    "objectID": "blog/project2/index.html#blueprinty-case-study",
    "href": "blog/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\nimport statsmodels.api as sm\nimport math\nfrom scipy.special import gammaln\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n\n\n\nblueprint = pd.read_csv(\"/Users/kaistern/Documents/MGTA_Data/MGTA_Marketing_Analytics/quarto_website/blog/project2/blueprinty.csv\")\n\nairbnb = pd.read_csv(\"/Users/kaistern/Documents/MGTA_Data/MGTA_Marketing_Analytics/quarto_website/blog/project2/airbnb.csv\")\n\n\ncusts = blueprint[blueprint['iscustomer']==1]\nnon_custs = blueprint[blueprint['iscustomer']==0]\n\ncust_mean = custs['patents'].mean()\nnon_cust_mean = non_custs['patents'].mean()\n\nprint(f'The mean number of patents for customers is {cust_mean} while the mean for non-patients is {non_cust_mean}')\n\nThe mean number of patents for customers is 4.133056133056133 while the mean for non-patients is 3.4730127576054954\n\n\n\nplt.figure(figsize=(8, 5))\nplt.hist(custs['patents'], bins=30, color='skyblue', edgecolor='black')\nplt.title(\"Patent Distribution - Customers\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(8, 5))\nplt.hist(non_custs['patents'], bins=30, color='salmon', edgecolor='black')\nplt.title(\"Patent Distribution - Non-Customers\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe histograms show that both customers and non-customers have right-skewed distributions of patent counts, with most values concentrated below 6. Customers peak around 4 patents, while non-customers peak slightly lower, around 2–3. The mean number of patents is higher for customers (4.13) than for non-customers (3.47), suggesting that customers tend to be slightly more innovation-active. Despite a larger sample size among non-customers, the customer group shows a modest upward shift in patent ownership.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nplt.figure(figsize=(8, 5))\nsns.countplot(data=blueprint, x='region', hue='iscustomer')\nplt.title(\"Customer vs Non-Customer Counts by Region\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe regional distribution of customers and non-customers is clearly imbalanced. The Northeast region stands out with more customers than non-customers, while all other regions have significantly fewer customers. This suggests that customer status is not randomly distributed across regions and regional factors may influence Blueprinty selection.\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=blueprint, x='iscustomer', y='age')\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Customer Status (0 = Non-Customer, 1 = Customer)\")\nplt.ylabel(\"Age\")\nplt.xticks([0, 1], ['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe boxplot shows that customers tend to be slightly older than non-customers on average, with a modest upward shift in median age. Both groups have similar spreads and ranges, though customers show slightly higher values at the upper quartile. This indicates age may play a small but non-negligible role in customer selection.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for independent observations ( Y_1, , Y_n (_i) ) is:\n\\[\n\\mathcal{L}(\\beta) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!} = \\prod_{i=1}^n \\frac{e^{-e^{X_i^\\top \\beta}} \\left(e^{X_i^\\top \\beta}\\right)^{Y_i}}{Y_i!}\n\\]\nTaking the log of the likelihood gives the log-likelihood:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ Y_i X_i^\\top \\beta - e^{X_i^\\top \\beta} - \\log(Y_i!) \\right]\n\\]\nI implement this function below in python.\n\ndef log_factorial(y):\n    return np.array([np.sum(np.log(np.arange(1, int(yi)+1))) if yi &gt; 0 else 0 for yi in y])\n\ndef poisson_log_likelihood(lambd, y):\n    y = np.asarray(y)\n    n = len(y)\n\n    log_fact = log_factorial(y)\n    log_likelihood = -n * lambd + np.sum(y * np.log(lambd)) - np.sum(log_fact)\n    return log_likelihood\n\n\nY = blueprint['patents'].values  \nlambda_vals = np.linspace(0.1, 10, 200)  \n\n\nlog_likelihoods = [poisson_log_likelihood(lam, Y) for lam in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_likelihoods, label='Log-Likelihood')\nplt.axvline(lambda_vals[np.argmax(log_likelihoods)], color='red', linestyle='--', label='MLE λ')\nplt.title(\"Poisson Log-Likelihood as a Function of λ\")\nplt.xlabel(\"λ (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe log-likelihood curve rises sharply at low values of λ, peaks around the sample mean of patent counts (approximately 4), and then gradually declines reflecting the typical shape of a Poisson likelihood. The red dashed line marks the maximum likelihood estimate for λ, which corresponds to the value that best explains the observed data under the Poisson model.\n\nY = blueprint['patents'].values\n\nobjective = lambda lam: -poisson_log_likelihood(lam, Y)\n\nresult = minimize_scalar(objective, bounds=(0.01, 10), method='bounded')\n\nmle_lambda = result.x\nprint(f\"MLE for λ: {mle_lambda:.4f}\")\n\nMLE for λ: 3.6847\n\n\nThe MLE is 3.6847 which is aligns with the graphical output above and makes sense given our understanding of the data up to this point.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\ndef poisson_regression_log_likelihood(beta, X, y):\n    beta = np.atleast_1d(beta)\n    X = np.asarray(X)\n    y = np.asarray(y)\n\n    eta = X @ beta\n    lambdas = np.array([math.exp(val) for val in eta])\n\n    log_fact = gammaln(y + 1)  # log(y!) using gamma function\n\n    return np.sum(y * eta - lambdas - log_fact)\n\n\ndef poisson_loglik(beta, X, y):\n    eta = X @ beta\n    return sum(y[i]*eta[i] - math.exp(min(eta[i], 700)) - gammaln(y[i]+1) for i in range(len(y)))\n\n\ndef fit_poisson(df, y_col, features):\n    X = sm.add_constant(df[features]).values\n    y = df[y_col].values\n    neg_ll = lambda b: -poisson_loglik(b, X, y)\n    beta0 = np.zeros(X.shape[1])\n\n    res = minimize(neg_ll, beta0, method='Nelder-Mead')\n    if not res.success:\n        res = minimize(neg_ll, beta0, method='BFGS')\n\n    beta_hat = res.x\n    n = len(beta_hat)\n    eps = 1e-5\n    hess = np.zeros((n, n))\n\n    for i in range(n):\n        for j in range(i, n):\n            di, dj = np.zeros(n), np.zeros(n)\n            di[i], dj[j] = eps, eps\n            f = neg_ll(beta_hat)\n            f1 = neg_ll(beta_hat + di)\n            f2 = neg_ll(beta_hat + dj)\n            f12 = neg_ll(beta_hat + di + dj)\n            hess[i, j] = (f12 - f1 - f2 + f) / eps**2\n            hess[j, i] = hess[i, j]\n\n    try:\n        se = np.sqrt(np.diag(np.linalg.inv(hess)))\n    except:\n        se = np.full(n, np.nan)\n\n    summary = pd.DataFrame({'Coefficient': beta_hat, 'Std. Error': se}, index=sm.add_constant(df[features]).columns)\n\n    return {\n        'coefficients': beta_hat,\n        'std_errors': se,\n        'summary': summary,\n        'convergence': res.success,\n        'log_likelihood': -res.fun\n    }\n\n\ndf = blueprint.copy()\ndf['age_squared'] = df['age'] ** 2\ndf = pd.get_dummies(df, columns=['region'], drop_first=True)\nfeatures = ['age', 'age_squared', 'iscustomer'] + [col for col in df.columns if col.startswith('region_')]\n\nresult = fit_poisson(df, 'patents', features)\nprint(result['summary'])\n\n                  Coefficient  Std. Error\nconst               -0.329507    0.179246\nage                  0.137006    0.013514\nage_squared         -0.002764    0.000249\niscustomer           0.214490    0.030893\nregion_Northeast    -0.013539    0.043277\nregion_Northwest    -0.053201    0.053562\nregion_South         0.022149    0.052401\nregion_Southwest     0.042996    0.046625\n\n\n\nfeatures = ['age', 'age_squared', 'iscustomer'] + [col for col in df.columns if col.startswith('region_')]\nX = sm.add_constant(df[features]).astype(float)  \ny = df['patents'].astype(float)                  \n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nresult_glm = model.fit()\n\nprint(result_glm.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 09 Jun 2025   Deviance:                       2143.3\nTime:                        12:08:03   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst               -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage                  0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared         -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer           0.2076      0.031      6.719      0.000       0.147       0.268\nregion_Northeast     0.0292      0.044      0.669      0.504      -0.056       0.115\nregion_Northwest    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nregion_South         0.0566      0.053      1.074      0.283      -0.047       0.160\nregion_Southwest     0.0506      0.047      1.072      0.284      -0.042       0.143\n====================================================================================\n\n\nThe Poisson regression results suggest that age, age squared, and iscustomer status are significant predictors of the number of patents held, while regional differences are not statistically significant. Specifically, age has a positive coefficient (0.149), but the negative coefficient on age_squared (−0.003) indicates a concave relationship: the expected patent count increases with age at a decreasing rate. Being a customer is associated with a ~21% increase in expected patents (exp(0.208) ≈ 1.23), and this effect is highly significant. The intercept is also significant, while all regional indicators have large p-values (p &gt; 0.28), suggesting no strong regional differences in patent counts after accounting for other variables. The pseudo R-squared (0.136) indicates a modest fit.\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\ny_pred_0 = result_glm.predict(X_0)\ny_pred_1 = result_glm.predict(X_1)\n\ndiff = y_pred_1 - y_pred_0\naverage_effect = diff.mean()\n\nprint(f\"Average marginal effect of Blueprinty's software on expected patent count: {average_effect:.3f}\")\n\nAverage marginal effect of Blueprinty's software on expected patent count: 0.793\n\n\nOn average, being a Blueprinty customer is associated with 0.793 more expected patents per firm, holding all other characteristics constant. This suggests a substantive positive impact of the software on firms’ innovation output, assuming the model is well-specified and the number of patents is a good outcome measure."
  },
  {
    "objectID": "blog/project2/index.html#airbnb-case-study",
    "href": "blog/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\ndf = airbnb.copy()  \ndf['last_scraped'] = pd.to_datetime(df['last_scraped'])\ndf['host_since'] = pd.to_datetime(df['host_since'])\ndf['days'] = (df['last_scraped'] - df['host_since']).dt.days\n\nvars_to_keep = ['days', 'room_type', 'bathrooms', 'bedrooms', 'price',\n                'number_of_reviews', 'review_scores_cleanliness',\n                'review_scores_location', 'review_scores_value', 'instant_bookable']\ndf = df.dropna(subset=vars_to_keep)\n\ndf['instant_bookable'] = (df['instant_bookable'] == 't').astype(int)\n\ndf = pd.get_dummies(df, columns=['room_type'], drop_first=True)\n\n\nsns.histplot(df['number_of_reviews'], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram above shows that the majority of Airbnb listings have very few reviews, with a sharp right skew. Most units have fewer than 20 reviews, and a small number of listings have over 100, suggesting that a small subset of units receives a disproportionately high number of bookings (as proxied by reviews). This kind of distribution is typical in count data and justifies the use of a Poisson model for further analysis.\n\nsns.boxplot(x='room_type', y='number_of_reviews', data=airbnb)\nplt.title(\"Reviews by Room Type\")\nplt.show()\n\n\n\n\n\n\n\n\nThis boxplot shows the distribution of review counts across different room types. Entire homes/apartments and private rooms tend to receive more reviews on average than shared rooms, though all three room types have long upper tails indicating occasional listings with very high activity. The median number of reviews is relatively low across all categories, but shared rooms appear to have the fewest reviews overall, suggesting lower demand or usage compared to other room types.\n\nsns.scatterplot(x='price', y='number_of_reviews', data=df)\nplt.title(\"Reviews vs Price\")\nplt.show()\n\n\n\n\n\n\n\n\nThis scatterplot shows a strong negative relationship between price and number of reviews, suggesting that higher-priced listings tend to receive fewer bookings,as proxied by reviews. Most listings are clustered at lower price points with higher review counts, while units priced above ~$500 see a sharp drop-off in review volume. This indicates that guests may be more price-sensitive and that extremely high-priced units are rarely booked.\n\nfeatures = ['days', 'bathrooms', 'bedrooms', 'price',\n            'review_scores_cleanliness', 'review_scores_location', 'review_scores_value',\n            'instant_bookable',\n            'room_type_Private room', 'room_type_Shared room']\n\nX = sm.add_constant(df[features]).astype('float')\ny = df['number_of_reviews'].astype('float')\n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nresult = model.fit()\nprint(result.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30140\nModel:                            GLM   Df Residuals:                    30129\nModel Family:                 Poisson   Df Model:                           10\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -4.9041e+05\nDate:                Mon, 09 Jun 2025   Deviance:                   8.5945e+05\nTime:                        12:08:03   Pearson chi2:                 1.18e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.9655\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                         2.9427      0.017    176.920      0.000       2.910       2.975\ndays                          0.0005   1.86e-06    280.430      0.000       0.001       0.001\nbathrooms                    -0.1134      0.004    -30.067      0.000      -0.121      -0.106\nbedrooms                      0.0757      0.002     37.214      0.000       0.072       0.080\nprice                     -4.302e-05   8.27e-06     -5.199      0.000   -5.92e-05   -2.68e-05\nreview_scores_cleanliness     0.1110      0.002     73.159      0.000       0.108       0.114\nreview_scores_location       -0.0815      0.002    -50.403      0.000      -0.085      -0.078\nreview_scores_value          -0.0911      0.002    -49.311      0.000      -0.095      -0.087\ninstant_bookable              0.4591      0.003    157.293      0.000       0.453       0.465\nroom_type_Private room        0.0192      0.003      7.025      0.000       0.014       0.025\nroom_type_Shared room        -0.1152      0.009    -13.318      0.000      -0.132      -0.098\n=============================================================================================\n\n\nThis Poisson regression model examines how various listing characteristics influence the number of reviews on Airbnb, used here as a proxy for bookings. The results indicate that listings active for a longer period (days) tend to accumulate more reviews, as expected. Larger listings, as measured by the number of bedrooms, are also associated with more bookings, while listings with more bathrooms are surprisingly linked to fewer reviews, possibly reflecting reduced demand for larger or more expensive properties. Price has a small but statistically significant negative effect, confirming that higher nightly rates are generally associated with fewer bookings.\nAmong review-based quality metrics, cleanliness stands out with a strong positive effect, indicating that guests reward cleaner properties with more bookings. Interestingly, higher scores for location and value are associated with fewer reviews, which may reflect unobserved factors. For example, budget properties in less central locations may be rated as better “value” but still attract fewer bookings overall. The ability to book instantly is associated with a substantial increase in review volume, suggesting that convenience is a major driver of guest behavior. Lastly, compared to entire homes, private rooms receive slightly more reviews, while shared rooms receive significantly fewer, highlighting differences in demand by room type."
  }
]