[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nKai Stern\n\n\nApr 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 1\n\n\nAn intro project.\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was designed to understand how changes in the perceived “price” of giving affect charitable behavior. Matching grant letters offered donors the opportunity to have their gifts matched by a leadership donor at varying rates, either $1:$1, $2:$1, or $3:$1. This effectively lowers the cost of providing a dollar to the charity. Additionally, the researchers randomized the stated size of the matching grant (e.g., $25,000 or $100,000) and varied the suggested donation amount based on past donor behavior. This allowed for a nuanced investigation of how framing and financial incentives influence both the likelihood of giving and the amount given.\nThe experiment was embedded in a real-world fundraising campaign conducted by a politically liberal nonprofit organization, making it a natural field experiment rather than a lab-based or hypothetical one. The study found that the mere presence of a matching grant substantially increased response rates and average donations. However, increasing the match ratio above 1:1 had no significant additional effect, suggesting that psychological framing, rather than purely economic incentives, plays a key role in donor decision-making. These findings have important implications for fundraising strategy, behavioral economics, and the broader understanding of prosocial behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was designed to understand how changes in the perceived “price” of giving affect charitable behavior. Matching grant letters offered donors the opportunity to have their gifts matched by a leadership donor at varying rates, either $1:$1, $2:$1, or $3:$1. This effectively lowers the cost of providing a dollar to the charity. Additionally, the researchers randomized the stated size of the matching grant (e.g., $25,000 or $100,000) and varied the suggested donation amount based on past donor behavior. This allowed for a nuanced investigation of how framing and financial incentives influence both the likelihood of giving and the amount given.\nThe experiment was embedded in a real-world fundraising campaign conducted by a politically liberal nonprofit organization, making it a natural field experiment rather than a lab-based or hypothetical one. The study found that the mere presence of a matching grant substantially increased response rates and average donations. However, increasing the match ratio above 1:1 had no significant additional effect, suggesting that psychological framing, rather than purely economic incentives, plays a key role in donor decision-making. These findings have important implications for fundraising strategy, behavioral economics, and the broader understanding of prosocial behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.describe()\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# Define variables to test\nvariables = {\n    \"pwhite\": \"Proportion White\",\n    \"pblack\": \"Proportion Black\",\n    \"page18_39\": \"Proportion Age 18-39\",\n    \"ave_hh_sz\": \"Average Household Size\"\n}\n\nresults = []\n\nfor var, label in variables.items():\n    # Subset data\n    treat = df[df[\"treatment\"] == 1][var].dropna()\n    control = df[df[\"treatment\"] == 0][var].dropna()\n\n    # Manual t-test\n    diff = treat.mean() - control.mean()\n    se = np.sqrt(treat.var(ddof=1)/len(treat) + control.var(ddof=1)/len(control))\n    t_stat = diff / se\n    df_total = len(treat) + len(control) - 2\n    p_val_ttest = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=df_total))\n\n    # Regression\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    coef = model.params[\"treatment\"]\n    p_val_reg = model.pvalues[\"treatment\"]\n\n    results.append({\n        \"Variable\": label,\n        \"Diff (Treat - Control)\": diff,\n        \"T-test p-value\": p_val_ttest,\n        \"Regression Coef\": coef,\n        \"Regression p-value\": p_val_reg\n    })\n\npd.DataFrame(results)\n\n\n\n\n\n\n\n\nVariable\nDiff (Treat - Control)\nT-test p-value\nRegression Coef\nRegression p-value\n\n\n\n\n0\nProportion White\n-0.000913\n0.576130\n-0.000913\n0.575308\n\n\n1\nProportion Black\n0.000129\n0.922294\n0.000129\n0.921935\n\n\n2\nProportion Age 18-39\n-0.000124\n0.901123\n-0.000124\n0.901029\n\n\n3\nAverage Household Size\n0.003012\n0.410313\n0.003012\n0.409801\n\n\n\n\n\n\n\nI tested four demographic variables: proportion white, proportion black, proportion aged 18–39, and average household size for balance between treatment and control groups using both two-sample t-tests and simple linear regressions. In all cases, the p-values were well above the 0.05 threshold, indicating no statistically significant differences between groups. Additionally, the t-test differences and regression coefficients matched exactly, confirming consistency between methods. These results suggest that the randomization was successful and that the treatment and control groups are comparable on these baseline characteristics, supporting the validity of subsequent causal inferences."
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n# Calculate donation rates\nresponse_rates = df.groupby('treatment')['gave'].mean()\nlabels = ['Control', 'Treatment']\nvalues = response_rates.values\n\n# Plot\nplt.figure(figsize=(6, 4))\nbars = plt.bar(labels, values, edgecolor='black')\nplt.ylabel('Proportion Donated')\nplt.title('Donation Response Rate by Group')\n\nText(0.5, 1.0, 'Donation Response Rate by Group')\n\n\n\n\n\n\n\n\n\nThe bar charts demonstrate an approximately 5% difference between the proportions of donors in the treatment and control groups. The control group is made up of around 17% of donors while the treatment group is around 22%.\n\n# Subset data\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_control = df[df['treatment'] == 0]['gave']\n\n# Means for reference\nmean_treat = gave_treat.mean()\nmean_control = gave_control.mean()\ndiff = mean_treat - mean_control\n\n# Manual t-test for binary outcome\nn1, n2 = len(gave_treat), len(gave_control)\np1, p2 = mean_treat, mean_control\nvar1 = p1 * (1 - p1)\nvar2 = p2 * (1 - p2)\nse = np.sqrt(var1/n1 + var2/n2)\nt_stat = diff / se\ndf_total = n1 + n2 - 2\np_value_t = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=df_total))\n\n# Regression: gave ~ treatment\nreg = smf.ols('gave ~ treatment', data=df).fit()\nreg_coef = reg.params['treatment']\nreg_pval = reg.pvalues['treatment']\n\n# Display results\n{\n    \"Control Mean\": mean_control,\n    \"Treatment Mean\": mean_treat,\n    \"Difference\": diff,\n    \"T-statistic\": t_stat,\n    \"T-test p-value\": p_value_t,\n    \"Regression Coefficient\": reg_coef,\n    \"Regression p-value\": reg_pval\n}\n\n{'Control Mean': np.float64(0.017858212980164198),\n 'Treatment Mean': np.float64(0.02203856749311295),\n 'Difference': np.float64(0.00418035451294875),\n 'T-statistic': np.float64(3.209540056375026),\n 'T-test p-value': np.float64(0.001330312711992132),\n 'Regression Coefficient': np.float64(0.004180354512949392),\n 'Regression p-value': np.float64(0.0019274025949017077)}\n\n\nI tested whether people who received a fundraising letter with a matching donation offer were more likely to donate compared to those who received a standard letter. A t-test shows that the treatment group had a statistically significantly higher donation rate than the control group. This result is confirmed by a linear regression, where the coefficient on the treatment variable closely matches the observed difference in means and is statistically significant.\nIn practical terms, just mentioning that a matching donation is available makes people more likely to give. Even a small change in framing (e.g., adding a single paragraph about a match) meaningfully influences behavior. This supports the idea that people are motivated not just by the financial impact of their gift, but by social signals like the opportunity to “unlock” funds from another donor. It also reinforces how seemingly minor tweaks in messaging can have outsized effects in fundraising campaigns.\n\ndef summarize_model(model, dep_var=\"y\"):\n    coef = model.params\n    se = model.bse\n    pval = model.pvalues\n    zval = model.tvalues  # also t-values for OLS\n    ci = model.conf_int()\n    n = int(model.nobs)\n\n    is_probit = hasattr(model, 'prsquared')  # True for Probit/Logit, not for OLS\n\n    if is_probit:\n        print(f\"### Probit Regression: {dep_var} ~ [predictors]\")\n        print(f\"**Sample size:** {n}\")\n        print(f\"**Pseudo R-squared:** {model.prsquared:.4f}\")\n        print(f\"**Log-likelihood:** {model.llf:.1f}\")\n        print(f\"**Model significance (LLR p-value):** {model.llr_pvalue:.4f}\")\n    else:\n        print(f\"### OLS Regression: {dep_var} ~ [predictors]\")\n        print(f\"**Sample size:** {n}\")\n        print(f\"**R-squared:** {model.rsquared:.4f}\")\n\n    print(\"\\n| Variable   | Coefficient | Std. Error | z/t-value | p-value | 95% CI            |\")\n    print(\"|------------|-------------|------------|-----------|---------|-------------------|\")\n\n    for var in coef.index:\n        ci_lower = ci.loc[var, 0]\n        ci_upper = ci.loc[var, 1]\n        print(f\"| {var:&lt;10} | {coef[var]:&gt;11.4f} | {se[var]:&gt;10.4f} | {zval[var]:&gt;9.2f} | {pval[var]:&gt;7.4f} | [{ci_lower:.3f}, {ci_upper:.3f}] |\")\n\n\n# Probit regression\nprobit_model = smf.probit('gave ~ treatment', data=df).fit()\nsummarize_model(probit_model, 'gave')\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n### Probit Regression: gave ~ [predictors]\n**Sample size:** 50083\n**Pseudo R-squared:** 0.0010\n**Log-likelihood:** -5030.5\n**Model significance (LLR p-value):** 0.0017\n\n| Variable   | Coefficient | Std. Error | z/t-value | p-value | 95% CI            |\n|------------|-------------|------------|-----------|---------|-------------------|\n| Intercept  |     -2.1001 |     0.0233 |    -90.07 |  0.0000 | [-2.146, -2.054] |\n| treatment  |      0.0868 |     0.0279 |      3.11 |  0.0019 | [0.032, 0.141] |\n\n\nI estimated a probit regression to examine whether assignment to the treatment group receiving a matching donation offer increased the likelihood that an individual made a charitable donation. The dependent variable was “gave”, a binary indicator equal to 1 if any donation was made. The key explanatory variable was treatment, equal to 1 for individuals who received a matching offer.\nThe probit model results show a positive and statistically significant coefficient of 0.0868 on the treatment variable (p = 0.002). This matches the findings reported in Table 3, Column 1 of Karlan & List (2007), where the coefficient on treatment is also approximately 0.087 and highly significant.\nThis result confirms that simply including a matching grant offer in a fundraising letter increases the probability of giving. The significance of the coefficient suggests that the effect is not due to chance. While the magnitude of the effect is modest in absolute terms, its consistency across t-tests, OLS, and probit models strengthens the evidence that framing a donation as being matched can meaningfully influence donor behavior.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Limit to only people who received a treatment letter (match offered)\ntreated = df[df['treatment'] == 1]\n\n# Get donation responses by match ratio\ngave_1to1 = treated[treated['ratio'] == 1]['gave']\ngave_2to1 = treated[treated['ratio'] == 2]['gave']\ngave_3to1 = treated[treated['ratio'] == 3]['gave']\n\n# Calculate means for each group\nmean_1 = gave_1to1.mean()\nmean_2 = gave_2to1.mean()\nmean_3 = gave_3to1.mean()\n\n# Run t-tests\nt_2_vs_1 = ttest_ind(gave_2to1, gave_1to1)\nt_3_vs_2 = ttest_ind(gave_3to1, gave_2to1)\nt_3_vs_1 = ttest_ind(gave_3to1,gave_1to1)\n\n{\n    \"1:1 Match Rate\": mean_1,\n    \"2:1 Match Rate\": mean_2,\n    \"3:1 Match Rate\": mean_3,\n    \"p-value: 2:1 vs 1:1\": t_2_vs_1.pvalue,\n    \"p-value: 3:1 vs 2:1\": t_3_vs_2.pvalue,\n    \"p-value: 3:1 vs 1:1\": t_3_vs_1.pvalue\n}\n\n{'1:1 Match Rate': np.float64(0.020749124225276205),\n '2:1 Match Rate': np.float64(0.0226333752469912),\n '3:1 Match Rate': np.float64(0.022733399227244138),\n 'p-value: 2:1 vs 1:1': np.float64(0.33453168549723933),\n 'p-value: 3:1 vs 2:1': np.float64(0.9600305283739325),\n 'p-value: 3:1 vs 1:1': np.float64(0.31010466370866724)}\n\n\nTo assess whether larger match ratios increase donation rates, I performed a series of t-tests comparing response rates among individuals assigned to 1:1, 2:1, and 3:1 match treatment groups. The donation rate was approximately 2.07% for the 1:1 group, 2.26% for 2:1, and 2.27% for 3:1. Although there is a small increase in the average response rate as the match ratio increases, none of the differences are statistically significant: the p-value comparing 2:1 to 1:1 is 0.335, and for 3:1 vs. 2:1 it is 0.960. Even the largest comparison (3:1 vs. 1:1) yields a p-value of 0.310, far above the conventional 0.05 threshold for significance.\nThese results support the authors’ “figures suggest” comment on page 8 of the paper: while the raw response rates rise slightly with higher match ratios, the increases are not statistically meaningful. This suggests that once a match is offered, increasing the size of that match even up to 3:1 does not further motivate donors to give\n\ntreated = df[df['treatment'] == 1].copy()\n\n# Create dummy variables \ntreated['ratio1'] = (treated['ratio'] == 1).astype(int)\ntreated['ratio2'] = (treated['ratio'] == 2).astype(int)\ntreated['ratio3'] = (treated['ratio'] == 3).astype(int)\n\n# Regression: gave ~ ratio2 + ratio3 \nratio_reg = smf.ols('gave ~ ratio2 + ratio3', data=treated).fit()\n\nsummarize_model(ratio_reg, 'gave')\n\n### OLS Regression: gave ~ [predictors]\n**Sample size:** 33396\n**R-squared:** 0.0000\n\n| Variable   | Coefficient | Std. Error | z/t-value | p-value | 95% CI            |\n|------------|-------------|------------|-----------|---------|-------------------|\n| Intercept  |      0.0207 |     0.0014 |     14.91 |  0.0000 | [0.018, 0.023] |\n| ratio2     |      0.0019 |     0.0020 |      0.96 |  0.3383 | [-0.002, 0.006] |\n| ratio3     |      0.0020 |     0.0020 |      1.01 |  0.3133 | [-0.002, 0.006] |\n\n\nTo assess whether the size of the match ratio influenced donation behavior, I regressed the binary outcome variable gave on dummy variables for 2:1 and 3:1 match ratios, using the 1:1 match as the baseline. The results show that neither the 2:1 nor 3:1 match ratios had a statistically significant effect on the likelihood of donating compared to the 1:1 match. The coefficient for ratio2 was 0.0019 (p = 0.338), and for ratio3 it was 0.0020 (p = 0.313), both with confidence intervals that include zero. The intercept, representing the baseline 1:1 match group, was 0.0207, consistent with earlier descriptive statistics. These findings confirm that increasing the size of the match beyond 1:1 does not meaningfully increase donation rates, reinforcing the paper’s conclusion that the presence of a match offer matters more than its size.\n\nmean_1 = gave_1to1.mean()\nmean_2 = gave_2to1.mean()\nmean_3 = gave_3to1.mean()\n\n# Raw differences\nraw_diff_2_vs_1 = mean_2 - mean_1\nraw_diff_3_vs_2 = mean_3 - mean_2\n\n# From regression results\ncoef_2 = ratio_reg.params['ratio2']\ncoef_3 = ratio_reg.params['ratio3']\n\n# Fitted differences (1:1 is baseline = intercept)\nfitted_diff_2_vs_1 = coef_2\nfitted_diff_3_vs_2 = coef_3 - coef_2\n\n{\n    \"Raw Difference (2:1 - 1:1)\": raw_diff_2_vs_1,\n    \"Raw Difference (3:1 - 2:1)\": raw_diff_3_vs_2,\n    \"Fitted Difference (2:1 - 1:1)\": fitted_diff_2_vs_1,\n    \"Fitted Difference (3:1 - 2:1)\": fitted_diff_3_vs_2\n}\n\n{'Raw Difference (2:1 - 1:1)': np.float64(0.0018842510217149944),\n 'Raw Difference (3:1 - 2:1)': np.float64(0.00010002398025293902),\n 'Fitted Difference (2:1 - 1:1)': np.float64(0.0018842510217148354),\n 'Fitted Difference (3:1 - 2:1)': np.float64(0.00010002398025296178)}\n\n\nTo further assess the effect of increasing the match ratio on donation rates, I calculated both raw and regression-based differences in response rates. The raw data show a donation rate of 2.07% for the 1:1 group, 2.26% for the 2:1 group, and 2.27% for the 3:1 group. The difference between 2:1 and 1:1 is approximately 0.0019, while the difference between 3:1 and 2:1 is just 0.0001. These match almost exactly the fitted differences obtained from the regression coefficients (0.0019 and 0.0001, respectively). The consistency between the raw and model-based estimates reinforces the conclusion that larger match ratios do not meaningfully increase the likelihood of giving. The small and statistically insignificant differences confirm that the presence of a match matters, but its generosity does not seem to influence donor behavior beyond that initial effect.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# Bivariate regression: total amount donated ~ treatment\namount_reg = smf.ols('amount ~ treatment', data=df).fit()\nsummarize_model(amount_reg, 'amount')\n\n### OLS Regression: amount ~ [predictors]\n**Sample size:** 50083\n**R-squared:** 0.0001\n\n| Variable   | Coefficient | Std. Error | z/t-value | p-value | 95% CI            |\n|------------|-------------|------------|-----------|---------|-------------------|\n| Intercept  |      0.8133 |     0.0674 |     12.06 |  0.0000 | [0.681, 0.945] |\n| treatment  |      0.1536 |     0.0826 |      1.86 |  0.0628 | [-0.008, 0.315] |\n\n\nTo assess whether offering a matching donation influenced not just the likelihood of giving but also the amount donated, I regressed the dollar amount of contributions on the treatment assignment indicator. The results show that the average donation in the control group was $0.81, as reflected in the intercept. Individuals in the treatment group gave $0.15 more on average, though this difference is not statistically significant at the 5% level (p = 0.063). While the coefficient is positive and suggestive of a modest increase in donation size, the lack of statistical significance means we cannot confidently conclude that the treatment had a reliable effect on donation amounts. This suggests that the primary effect of the matching offer was to increase participation, rather than the average amount given per donor.\n\ndonors = df[df['gave']&gt;0]\n\namount_reg2 = smf.ols('amount ~ treatment', data=donors).fit()\namount_reg2.summary()\n\nsummarize_model(amount_reg2, 'amount')\n\n### OLS Regression: amount ~ [predictors]\n**Sample size:** 1034\n**R-squared:** 0.0003\n\n| Variable   | Coefficient | Std. Error | z/t-value | p-value | 95% CI            |\n|------------|-------------|------------|-----------|---------|-------------------|\n| Intercept  |     45.5403 |     2.4234 |     18.79 |  0.0000 | [40.785, 50.296] |\n| treatment  |     -1.6684 |     2.8724 |     -0.58 |  0.5615 | [-7.305, 3.968] |\n\n\nTo understand whether the treatment influenced how much people donated, conditional on having donated, I restricted the dataset to individuals who made a donation (gave &gt; 0) and regressed amount on the treatment indicator. The results show that the average donation amount in the control group was approximately $45.54, while the treatment group gave $1.67 less on average, according to the regression coefficient. However, this difference is not statistically significant (p = 0.561), and the 95% confidence interval includes zero (−7.31 to 3.97). This indicates that among those who did choose to donate, being offered a matching grant did not significantly affect the donation amount.\n\n# Separate treatment and control donors\ntreat_donors = donors[donors['treatment'] == 1]['amount']\ncontrol_donors = donors[donors['treatment'] == 0]['amount']\n\n# Calculate means\nmean_treat = treat_donors.mean()\nmean_control = control_donors.mean()\n\n# Plot: Control Group\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.hist(control_donors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\nplt.axvline(mean_control, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean_control:.2f}')\nplt.title(\"Control Group Donations\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\n\n# Plot: Treatment Group\nplt.subplot(1, 2, 2)\nplt.hist(treat_donors, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.axvline(mean_treat, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean_treat:.2f}')\nplt.title(\"Treatment Group Donations\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese histograms show the distribution of donation amounts among individuals who gave, separated by treatment and control groups. While both distributions are right-skewed, the average donation is slightly lower in the treatment group ($43.87) than in the control group ($45.54), indicating that the match offer did not increase conditional donation amounts."
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate donation behavior: Bernoulli draws\n# 100,000 from control group, p = 0.018\n# 10,000 from treatment group, p = 0.022\nnp.random.seed(42)\ncontrol_draws = np.random.binomial(1, 0.018, 100000)\ntreat_draws = np.random.binomial(1, 0.022, 10000)\n\n# Calculate pointwise differences between treatment and control draws\ndiffs = treat_draws - control_draws[:10000]  # align sizes\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot cumulative average\nplt.figure(figsize=(8, 4))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences')\nplt.axhline(0.004, color='red', linestyle='--', label='True Difference (0.022 - 0.018)')\nplt.title(\"Law of Large Numbers: Cumulative Average of Simulated Differences\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot illustrates the Law of Large Numbers by showing how the cumulative average of simulated differences in donation behavior between treatment and control groups stabilizes as the sample size increases. Initially, the cumulative average fluctuates widely due to random variation in small samples, but as more observations accumulate, the average converges toward the true difference in population means: 0.004 (marked by the red dashed line). This visual evidence confirms that with a large enough sample, the observed difference in donation rates between treatment and control becomes a reliable estimate of the actual treatment effect.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulation setup\nnp.random.seed(42)\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\n# Function to simulate one sample difference\ndef simulate_diffs(n, p1, p2, reps=1000):\n    diffs = []\n    for _ in range(reps):\n        c = np.random.binomial(1, p1, n)\n        t = np.random.binomial(1, p2, n)\n        diffs.append(np.mean(t) - np.mean(c))\n    return np.array(diffs)\n\n# Generate and plot\nplt.figure(figsize=(12, 10))\n\nfor i, n in enumerate(sample_sizes, 1):\n    diffs = simulate_diffs(n, p_control, p_treatment)\n    plt.subplot(2, 2, i)\n    plt.hist(diffs, bins=30, edgecolor='black', alpha=0.7, density=True)\n    plt.axvline(0, color='black', linestyle='--', linewidth=1, label='Zero')\n    plt.axvline(0.004, color='red', linestyle='--', linewidth=2, label='True Diff = 0.004')\n    plt.title(f\"Sample Size = {n}\")\n    plt.xlabel(\"Difference in Means\")\n    plt.ylabel(\"Density\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese four histograms illustrate the Central Limit Theorem by showing the sampling distribution of the difference in means between treatment and control groups across varying sample sizes (50, 200, 500, and 1000). At smaller sample sizes (e.g., 50), the distribution is wide and irregular, and zero frequently appears near the center—indicating that we might observe no difference simply due to sampling variability. As the sample size increases, the distribution becomes more concentrated and symmetric, and the center of the distribution shifts closer to the true difference of 0.004 (indicated by the red dashed line). By the time we reach a sample size of 1000, the sampling distribution is tightly centered around the true effect, and zero lies in the tail rather than the center. This pattern highlights how increasing sample size leads to more precise and reliable estimates, reducing the likelihood that random chance obscures true treatment effects"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kai’s Stern",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "This is project 2\nx = 10 print(x)"
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nThe mean number of patents for customers is 4.133056133056133 while the mean for non-patients is 3.4730127576054954\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe histograms show that both customers and non-customers have right-skewed distributions of patent counts, with most values concentrated below 6. Customers peak around 4 patents, while non-customers peak slightly lower, around 2–3. The mean number of patents is higher for customers (4.13) than for non-customers (3.47), suggesting that customers tend to be slightly more innovation-active. Despite a larger sample size among non-customers, the customer group shows a modest upward shift in patent ownership.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\nThe regional distribution of customers and non-customers is clearly imbalanced. The Northeast region stands out with more customers than non-customers, while all other regions have significantly fewer customers. This suggests that customer status is not randomly distributed across regions and regional factors may influence Blueprinty selection.\n\n\n\n\n\n\n\n\n\nThe boxplot shows that customers tend to be slightly older than non-customers on average, with a modest upward shift in median age. Both groups have similar spreads and ranges, though customers show slightly higher values at the upper quartile. This indicates age may play a small but non-negligible role in customer selection.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for independent observations ( Y_1, , Y_n (_i) ) is:\n\\[\n\\mathcal{L}(\\beta) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!} = \\prod_{i=1}^n \\frac{e^{-e^{X_i^\\top \\beta}} \\left(e^{X_i^\\top \\beta}\\right)^{Y_i}}{Y_i!}\n\\]\nTaking the log of the likelihood gives the log-likelihood:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ Y_i X_i^\\top \\beta - e^{X_i^\\top \\beta} - \\log(Y_i!) \\right]\n\\]\nI implement this function below in python.\n\n\n\n\n\n\n\n\n\nThe log-likelihood curve rises sharply at low values of λ, peaks around the sample mean of patent counts (approximately 4), and then gradually declines reflecting the typical shape of a Poisson likelihood. The red dashed line marks the maximum likelihood estimate for λ, which corresponds to the value that best explains the observed data under the Poisson model.\n\n\nMLE for λ: 3.6847\n\n\nThe MLE is 3.6847 which is aligns with the graphical output above and makes sense given our understanding of the data up to this point.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n                  Coefficient  Std. Error\nconst               -0.329507    0.179246\nage                  0.137006    0.013514\nage_squared         -0.002764    0.000249\niscustomer           0.214490    0.030893\nregion_Northeast    -0.013539    0.043277\nregion_Northwest    -0.053201    0.053562\nregion_South         0.022149    0.052401\nregion_Southwest     0.042996    0.046625\n\n\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Tue, 06 May 2025   Deviance:                       2143.3\nTime:                        13:10:25   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst               -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage                  0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared         -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer           0.2076      0.031      6.719      0.000       0.147       0.268\nregion_Northeast     0.0292      0.044      0.669      0.504      -0.056       0.115\nregion_Northwest    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nregion_South         0.0566      0.053      1.074      0.283      -0.047       0.160\nregion_Southwest     0.0506      0.047      1.072      0.284      -0.042       0.143\n====================================================================================\n\n\nThe Poisson regression results suggest that age, age squared, and iscustomer status are significant predictors of the number of patents held, while regional differences are not statistically significant. Specifically, age has a positive coefficient (0.149), but the negative coefficient on age_squared (−0.003) indicates a concave relationship: the expected patent count increases with age at a decreasing rate. Being a customer is associated with a ~21% increase in expected patents (exp(0.208) ≈ 1.23), and this effect is highly significant. The intercept is also significant, while all regional indicators have large p-values (p &gt; 0.28), suggesting no strong regional differences in patent counts after accounting for other variables. The pseudo R-squared (0.136) indicates a modest fit.\n\n\nAverage marginal effect of Blueprinty's software on expected patent count: 0.793\n\n\nOn average, being a Blueprinty customer is associated with 0.793 more expected patents per firm, holding all other characteristics constant. This suggests a substantive positive impact of the software on firms’ innovation output, assuming the model is well-specified and the number of patents is a good outcome measure."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nThe mean number of patents for customers is 4.133056133056133 while the mean for non-patients is 3.4730127576054954\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe histograms show that both customers and non-customers have right-skewed distributions of patent counts, with most values concentrated below 6. Customers peak around 4 patents, while non-customers peak slightly lower, around 2–3. The mean number of patents is higher for customers (4.13) than for non-customers (3.47), suggesting that customers tend to be slightly more innovation-active. Despite a larger sample size among non-customers, the customer group shows a modest upward shift in patent ownership.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\nThe regional distribution of customers and non-customers is clearly imbalanced. The Northeast region stands out with more customers than non-customers, while all other regions have significantly fewer customers. This suggests that customer status is not randomly distributed across regions and regional factors may influence Blueprinty selection.\n\n\n\n\n\n\n\n\n\nThe boxplot shows that customers tend to be slightly older than non-customers on average, with a modest upward shift in median age. Both groups have similar spreads and ranges, though customers show slightly higher values at the upper quartile. This indicates age may play a small but non-negligible role in customer selection.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for independent observations ( Y_1, , Y_n (_i) ) is:\n\\[\n\\mathcal{L}(\\beta) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!} = \\prod_{i=1}^n \\frac{e^{-e^{X_i^\\top \\beta}} \\left(e^{X_i^\\top \\beta}\\right)^{Y_i}}{Y_i!}\n\\]\nTaking the log of the likelihood gives the log-likelihood:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ Y_i X_i^\\top \\beta - e^{X_i^\\top \\beta} - \\log(Y_i!) \\right]\n\\]\nI implement this function below in python.\n\n\n\n\n\n\n\n\n\nThe log-likelihood curve rises sharply at low values of λ, peaks around the sample mean of patent counts (approximately 4), and then gradually declines reflecting the typical shape of a Poisson likelihood. The red dashed line marks the maximum likelihood estimate for λ, which corresponds to the value that best explains the observed data under the Poisson model.\n\n\nMLE for λ: 3.6847\n\n\nThe MLE is 3.6847 which is aligns with the graphical output above and makes sense given our understanding of the data up to this point.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n                  Coefficient  Std. Error\nconst               -0.329507    0.179246\nage                  0.137006    0.013514\nage_squared         -0.002764    0.000249\niscustomer           0.214490    0.030893\nregion_Northeast    -0.013539    0.043277\nregion_Northwest    -0.053201    0.053562\nregion_South         0.022149    0.052401\nregion_Southwest     0.042996    0.046625\n\n\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Tue, 06 May 2025   Deviance:                       2143.3\nTime:                        13:10:25   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst               -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage                  0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared         -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer           0.2076      0.031      6.719      0.000       0.147       0.268\nregion_Northeast     0.0292      0.044      0.669      0.504      -0.056       0.115\nregion_Northwest    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nregion_South         0.0566      0.053      1.074      0.283      -0.047       0.160\nregion_Southwest     0.0506      0.047      1.072      0.284      -0.042       0.143\n====================================================================================\n\n\nThe Poisson regression results suggest that age, age squared, and iscustomer status are significant predictors of the number of patents held, while regional differences are not statistically significant. Specifically, age has a positive coefficient (0.149), but the negative coefficient on age_squared (−0.003) indicates a concave relationship: the expected patent count increases with age at a decreasing rate. Being a customer is associated with a ~21% increase in expected patents (exp(0.208) ≈ 1.23), and this effect is highly significant. The intercept is also significant, while all regional indicators have large p-values (p &gt; 0.28), suggesting no strong regional differences in patent counts after accounting for other variables. The pseudo R-squared (0.136) indicates a modest fit.\n\n\nAverage marginal effect of Blueprinty's software on expected patent count: 0.793\n\n\nOn average, being a Blueprinty customer is associated with 0.793 more expected patents per firm, holding all other characteristics constant. This suggests a substantive positive impact of the software on firms’ innovation output, assuming the model is well-specified and the number of patents is a good outcome measure."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\n\n\n\n\n\n\n\nThe histogram above shows that the majority of Airbnb listings have very few reviews, with a sharp right skew. Most units have fewer than 20 reviews, and a small number of listings have over 100, suggesting that a small subset of units receives a disproportionately high number of bookings (as proxied by reviews). This kind of distribution is typical in count data and justifies the use of a Poisson model for further analysis.\n\n\n\n\n\n\n\n\n\nThis boxplot shows the distribution of review counts across different room types. Entire homes/apartments and private rooms tend to receive more reviews on average than shared rooms, though all three room types have long upper tails indicating occasional listings with very high activity. The median number of reviews is relatively low across all categories, but shared rooms appear to have the fewest reviews overall, suggesting lower demand or usage compared to other room types.\n\n\n\n\n\n\n\n\n\nThis scatterplot shows a strong negative relationship between price and number of reviews, suggesting that higher-priced listings tend to receive fewer bookings,as proxied by reviews. Most listings are clustered at lower price points with higher review counts, while units priced above ~$500 see a sharp drop-off in review volume. This indicates that guests may be more price-sensitive and that extremely high-priced units are rarely booked.\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30140\nModel:                            GLM   Df Residuals:                    30129\nModel Family:                 Poisson   Df Model:                           10\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -4.9041e+05\nDate:                Tue, 06 May 2025   Deviance:                   8.5945e+05\nTime:                        13:10:26   Pearson chi2:                 1.18e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.9655\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                         2.9427      0.017    176.920      0.000       2.910       2.975\ndays                          0.0005   1.86e-06    280.430      0.000       0.001       0.001\nbathrooms                    -0.1134      0.004    -30.067      0.000      -0.121      -0.106\nbedrooms                      0.0757      0.002     37.214      0.000       0.072       0.080\nprice                     -4.302e-05   8.27e-06     -5.199      0.000   -5.92e-05   -2.68e-05\nreview_scores_cleanliness     0.1110      0.002     73.159      0.000       0.108       0.114\nreview_scores_location       -0.0815      0.002    -50.403      0.000      -0.085      -0.078\nreview_scores_value          -0.0911      0.002    -49.311      0.000      -0.095      -0.087\ninstant_bookable              0.4591      0.003    157.293      0.000       0.453       0.465\nroom_type_Private room        0.0192      0.003      7.025      0.000       0.014       0.025\nroom_type_Shared room        -0.1152      0.009    -13.318      0.000      -0.132      -0.098\n=============================================================================================\n\n\nThis Poisson regression model examines how various listing characteristics influence the number of reviews on Airbnb, used here as a proxy for bookings. The results indicate that listings active for a longer period (days) tend to accumulate more reviews, as expected. Larger listings, as measured by the number of bedrooms, are also associated with more bookings, while listings with more bathrooms are surprisingly linked to fewer reviews, possibly reflecting reduced demand for larger or more expensive properties. Price has a small but statistically significant negative effect, confirming that higher nightly rates are generally associated with fewer bookings.\nAmong review-based quality metrics, cleanliness stands out with a strong positive effect, indicating that guests reward cleaner properties with more bookings. Interestingly, higher scores for location and value are associated with fewer reviews, which may reflect unobserved factors. For example, budget properties in less central locations may be rated as better “value” but still attract fewer bookings overall. The ability to book instantly is associated with a substantial increase in review volume, suggesting that convenience is a major driver of guest behavior. Lastly, compared to entire homes, private rooms receive slightly more reviews, while shared rooms receive significantly fewer, highlighting differences in demand by room type."
  }
]